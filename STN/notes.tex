\documentclass[11pt]{article}
% Libraries.

\usepackage{dsfont}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{esint}
\usepackage[margin=3cm]{geometry}
%\usepackage{pgfplots}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{perpage}
\usepackage[dvipsnames, pdftex]{xcolor}
\usepackage{float}
\usepackage{xargs}
\usepackage{../raina}
\usepackage[
	colorinlistoftodos,
	prependcaption,
	textsize=tiny
]{todonotes}


\numberwithin{equation}{section}
\newcommand{\dm}[0]{\mathbf{\Phi}}

% Property settings.
\MakePerPage{footnote}
\pagestyle{headings}

% Attr.
\title{$\Delta$-STN on BNN \\ Research notes}
\author{Yuchen Wang}
\date{\today}

\begin{document}
    \maketitle
    \tableofcontents
    \newpage

\section{Bayesian Linear Regression}
\paragraph{Motivation}
Avoid the over-fitting problem of maximum likelihood (equivalent to MSE) and will also lead to automatic methods of determining model complexity using the training data alone.
\notation
Inputs: $X = \{x_1, \hdots, x_N\}$.\\
Targets: $t_1, \hdots, t_N$, which we group into a column vector $\vt \in \real^N$.\\
Basis functions: $\phi_j(\vx)$, groups into a column vector $\phi \in \real^M$.\\
Design matrix:
$\mathbf{\Phi}=\left(\begin{array}{cccc}\phi_{0}\left(\mathbf{x}_{1}\right) & \phi_{1}\left(\mathbf{x}_{1}\right) & \cdots & \phi_{M-1}\left(\mathbf{x}_{1}\right) \\ \phi_{0}\left(\mathbf{x}_{2}\right) & \phi_{1}\left(\mathbf{x}_{2}\right) & \cdots & \phi_{M-1}\left(\mathbf{x}_{2}\right) \\ \vdots & \vdots & \ddots & \vdots \\ \phi_{0}\left(\mathbf{x}_{N}\right) & \phi_{1}\left(\mathbf{x}_{N}\right) & \cdots & \phi_{M-1}\left(\mathbf{x}_{N}\right)\end{array}\right)$\\
\tb{Assumptions}: 
\begin{enumerate}
	\item The target variable $t$ is given by a deterministic function $y(\vx, \vw)$ with additive Gaussian noise so that
	\begin{equation}
	t = y(\vx, \vw) + \epsilon = \vw^T\phi(\vx)
	\end{equation}
	\item The inputs are drawn independently from the distribution
	\begin{equation} \label{lf}
	p(t | \vx, \vw, \beta) = \mc{N}(t|y(\vx, \vw), \beta^{-1})
	\end{equation}
	where $\beta$ is the \tb{precision} (inverse variance).
\end{enumerate}
The likelihood function (a function of the adjustable parameters $\vw$ and $\beta$):
\begin{equation}
p(\vt | X, \vw, \beta) = \prod_{n=1}^N \mc{N}(t_n|\vw^T\phi(\vx_n), \beta^{-1})
\end{equation}

\subsection{Parameter Distribution}
\paragraph{Introduction}
Introduce a prior probability distribution over the model parameters $\vw$.\\
\blue{For the moment, treat $\beta$ as a known constant.} \\
The corresponding conjugate prior of the likelihood function $p(\vt|\vw)$ defined by (\ref{lf}) is of the form
\begin{equation}
	p(\vw) = \mc{N}(\vw|\vm_0, \tb{S}_0)
\end{equation}
having mean $\vm_0$ and covariance $\tb{S}_0$.\\
The posterior is in the form
\begin{equation} \label{post}
	p(\vw|\vt) = \mc{N}(\vw|\vm_N, \bf{S}_N)
\end{equation}
where
\begin{align}
	\vm_N &= \tb{S}_N(\tb{S}_0^{-1}\vm_0 + \beta \mathbf{\Phi}^T\vt)\\
	\tb{S}_N^{-1} &= \mathbf{S}_0^{-1} + \beta \mathbf{\Phi}^T\mathbf{\Phi}
\end{align}
\remark
\red{Note that the mode of a Gaussian distribution coincides with its mean.} Thus
$$\vw_{MAP} = \vm_{N}$$

\remark
If we consider an infinitely broad prior $\tb{S}_0 = \alpha^{-1}\tb{I}$ with $\alpha \rightarrow 0$, the mean $\vm_N$ of the posterior distribution reduces the maximum likelihood value $\vw_{ML}$ given by
\begin{equation}
	\vw_{ML} = (\dm^T\dm)^{-1}\dm^T\vt
\end{equation}
\remark
Similarly, if $N = 0$, then the posterior distribution reverts to the prior.
\remark
Furthermore, if data points arrive sequentially, then the posterior distribution at any stage acts as the prior distribution for the subsequent data point, such that the new posterior distribution is again given by (\ref{post}).

\paragraph{Simplification}
Consider a zero-mean isotropic Gaussian governed by a single precision parameter $\alpha$ so that
\begin{equation}
	p(\vw|\alpha) = \mc{N}(\vw|\vo, \alpha^{-1}\tb{I})
\end{equation}
and the corresponding posterior distribution is then (\ref{post}) with
\begin{align}
	\vm_N &= \beta S_N\dm^T\vt\\
	\tb{S}_N^{-1} &= \alpha\tb{I} + \beta \mathbf{\Phi}^T\mathbf{\Phi}
\end{align}
The log of the posterior distribution is of the form
\begin{equation}
	\ln p(\vw|\vt) = -\frac{\beta}{2}\sum_{n=1}^N\{t_n - \vw^T\phi(\vx_n)\}^2 - \frac{\alpha}{2}\vw^T\vw + const
\end{equation}
Maximization of this function is equivalent to the minimization of MSE plus L2 regularization with $\lambda = \alpha / \beta$.


\subsection{Predictive distribution}
In practice, we are interested in making predictions of $y$ for new values of $\vx$ rather than the value of $\vw$. This requires that we evaluate the \tb{predictive distribution} define by
\begin{align}
	p(y|\vt, \alpha, \beta) &= \int p(y, \vw | \vt, \alpha, \beta) \, d\vw \\
	&= \int p(y|\vw, \vt, \alpha, \beta)p(\vw|\vt, \alpha, \beta)\, d\vw &&\text{(by Chain Rule)}\\
	&= \int p(y|\vw, \alpha, \beta)p(\vw|\vt, \alpha, \beta)\, d\vw &&\text{($y \perp \vt$)}
\end{align}
where $\vt$ is the vector of target values from the training set. We see that the above equation involves the convolution of two Gaussian distributions, and so making use of the result in (Figure \ref{p1}) we can see that the predictive distribution takes the form
\begin{align}
	p(y|\vt, \vx, \alpha, \beta) = \mc{N}(y|\vm^T_N\phi(\vx), \underbrace{\beta^{-1} + \phi(\vx)^T\tb{S}_N\phi(\vx)}_{\sigma^2_N(\vx)})
\end{align}
\begin{figure}[H] 
	\centering
	\includegraphics[scale=0.6]{p1}
	\caption{\label{p1}}
\end{figure}
\remark
Note that, as additional data points are observed, the posterior distribution becomes narrower. It can be shown that $\sigma^2_{N+1}(\vx) \leq \sigma^2_N(\vx)$. In the limit $N \rightarrow \infty$, the second term of $\sigma^2_N(\vx)$ goes to zero, and the variance of the predictive distribution arises solely from the additive noise governed by the parameter $\beta$.


\subsection{Equivalent kernel}
The posterior mean solution for the linear basis function model has an interesting interpretation that will set the stage for kernel methods, including Gaussian process.\\
\definition{equivalent kernel}
We see that the predictive mean can be written in the form
\begin{equation}
	y(\vx, \vm_N) = \vm^T_N\phi(\vx) = \beta\phi(\vx)^T\tb{S}_N\dm^T\vt = \sum_{n=1}^N\beta \phi(\vx)^T\tb{S}_N\phi(\vx_n)t_n
\end{equation}
Thus the mean of the predictive distribution at a point $\vx$ is given by a linear combination of the training set target variables $t_n$, so that we can write
\begin{equation}
	y(\vx, \vm_N) = \sum_{n=1}^N k(\vx, \vx_n)t_n
\end{equation}
where
\begin{equation}
	k(\vx, \vx') = \beta \phi(\vx)^T\tb{S}_N\phi(\vx')
\end{equation}
is known is the \tb{equivalent kernel (smoother matrix)}.

\remark
The mean is obtained by forming a weighted combination of the target values in which data points close to $x$ are given higher weight than point further removed from $x$.
\property
\begin{align}
	\cov[y(\vx), y(\vx')] &= \cov[\phi(\vx^T\vw, \vw^T\phi(\vx')] \\
	 &= \phi(\vx)^T\tb{S}_N\phi(\vx') \\
	 &= \beta^{-1}k(\vx, \vx')
\end{align}
\remark
The predictive mean at nearby points will be highly correlated, whereas for more distant pairs of points the correlation will be smaller.

\remark
Instead of introducing a set of basis functions which implicitly determines an equivalent kernel, we can \blue{instead define a localized kernel directly and use this to make predictions for new input vectors $\vx$, given the observed training $\vx$.} This leads to \tb{Gaussian process}.

\property
For all values of $\vx$,
\begin{equation}
	\sum_{n=1}^N k(\vx, \vx_n) = 1
\end{equation}

\property
The equivalent kernel can be expressed in the form of an inner product with respect to a vector $\psi(\vx)$ of nonlinear functions, so that
\begin{equation}
	k(\vx, \vz) = \psi(\vx)^T\psi(\vz)
\end{equation}
where $\psi(\vx) = \beta^{1/2}\tb{S}^{1/2}_N\phi(\vx)$.


\section{Bayesian Model Comparison}
There are two types of models
\begin{enumerate}
	\item The distribution is defined over the set of target values $\vt$, while the set of input values $X$ is assumed to be known.
	\item Define a joint distribution over $X$ and $\vt$.
\end{enumerate}
Suppose the data is generated from one of the above but we are uncertain about which one. The uncertainty is expressed through a prior probability distribution $p(\mc{M}_i)$. Given a training set $\mc{D}$, we wish to evaluate the posterior distribution
\begin{equation}
	p(\mc{M}_i|\mc{D}) \propto p(\mc{M}_i)p(\mc{D}|\mc{M}_i)
\end{equation}
\definition[mixture distribution]
Once we know the posterior distribution over models, the predictive distribution is given, from the sum and product rules, by
\begin{equation}
	p(t|\vx, \mc{D}) = \sum_{i=1}^L p(t|\vx, \mc{M}_i, \mc{D})p(\mc{M}_1 | D)
\end{equation}
This is an example of a \tb{mixture distribution} in which the overall predictive distribution is obtained by averaging the predictive distributions $p(t|x, \mc{M}_i, D)$ of individual models, weighted by the posterior probabilities $p(\mc{M}_i|D)$ of those models.
\definition[model selection]
A simple approximation to model averaging is to use the single most probable model alone to make predictions. This is known as \tb{model selection}.
\definition[model evidence]
For a model governed by a set of parameters $\vw$, the \tb{model evidence} is given, from the sum and product rules of probability, by
\begin{equation}
	p(\mc{D}|\mc{M}_i) = \int p(\mc{D}|\vw, \mc{M}_i)p(\vw | \mc{M}_i)\,d\vw
\end{equation}
We can obtain some insight into the model evidence by making a simple approximation to the integral over parameters. Consider first the case of a model having a single parameter $w .$ The posterior distribution over parameters is proportional to $p(\mathcal{D} | w) p(w),$ where we omit the dependence on the model $\mathcal{M}_{i}$ to keep the notation uncluttered. If we assume that the posterior distribution is sharply peaked around the most probable value $w_{\mathrm{MAP}},$ with width $\Delta w_{\text {posterior }},$ then we can approximate the integral by the value of the integrand at its maximum times the width of the peak. If we further assume that the prior is flat with width $\Delta w_{\text {prior }}$ so that $p(w)=1 / \Delta w_{\text {prior }}$ then we have
\begin{equation}
	p(\mathcal{D})=\int p(\mathcal{D} | w) p(w) \mathrm{d} w \simeq p\left(\mathcal{D} | w_{\mathrm{MAP}}\right) \frac{\Delta w_{\text {posterior }}}{\Delta w_{\text {prior }}}
\end{equation}
where we omit the $\mc{M}_i$ to keep the notation uncluttered.
\definition[evaluating the posterior distribution]
\begin{equation}
	p(\vw|\mc{D},\mc{M}_i) = \frac{p(\mc{D}|\vw, \mc{M}_i)p(\vw|\mc{M}_i)}{p(\mc{D}|\mc{M}_i)}
\end{equation}

\definition[Bayes factor]
The ratio of model evidences 
\begin{equation}
	K = \frac{p(\mc{D}|\mc{M}_1)}{p(\mc{D}|\mc{M}_2)}
\end{equation}
for two models is known as a \tb{Bayes factor}.\\
A value of $K > 1$ means that $M_1$ is more strongly supported by the data under consideration than $M_2$.

\remark
The use of Bayes factors is a Bayesian alternative to classical hypothesis testing.






















\end{document}


