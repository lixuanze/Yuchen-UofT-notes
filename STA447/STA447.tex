\documentclass[11pt]{article}
% Libraries.

\usepackage{dsfont}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{esint}
\usepackage[margin=3cm]{geometry}
%\usepackage{pgfplots}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{perpage}
\usepackage[dvipsnames, pdftex]{xcolor}
\usepackage{float}
\usepackage{xargs}
\usepackage{/Users/raina/Desktop/uoft-notes/raina}
\usepackage[
	colorinlistoftodos,
	prependcaption,
	textsize=tiny
]{todonotes}

\newcommand{\ur}[2]{{#1}^{({#2})}}
\newcommand{\dur}[3]{{#1}_{#2}^{({#3})}}
\renewcommand{\limit}[1]{\underset{{#1} \rightarrow \infty}{\lim}}
% Property settings.
\MakePerPage{footnote}
\pagestyle{headings}

% Attr.
\title{STA447\\ Lecture Notes}
\author{Yuchen Wang}
\date{\today}

\begin{document}
    \maketitle
    \tableofcontents
    \newpage
    \section{Preliminary}
    \theorem[\label{Dominated Convergence Theorem}Dominated Convergence Theorem]
    If $\limit{n} X_n = X$, and there is some random variable $Y$ with $E|Y|<\infty$ and $|X_n| \leq Y$ for all $n$, then $$\limit{n} E(X_n) = E(X)$$
    
    \definition[weak convergence]
    $X_n$ converge to $X$ weakly if 
    $$\forall \epsilon > 0, \limit{n}P(|X_n - X| \geq \epsilon) = 0$$
   
    \definition[strong convergence]
    $X_n$ converge to $X$ strongly if 
    $$P(\limit{n}X_n = X) = 1$$ 
    
    
    \theorem[\label{Law of Large Numbers}Law of Large Numbers]
    If the sequence $\{X_n\}$ is i.i.d. with common mean $m$, then the sequence $\frac{1}{n}\sum_{i=1}^n X_i$ converges to $m$ (both weakly and strongly), i.e.,
    $$\limit{n} \frac{1}{n}\sum_{i=1}^nX_i = m \quad w.p. 1$$ 


    \section{Markov Chain Probabilities}
    \notation
    $$P(X_{n+1} = j | X_n = i) = p_{ij}$$
    \definition[Markov chain] A (discrete time, discrete space, time homogeneous) \under{Markov chain} is specified by three ingredients:
    \begin{itemize}
    	\item A \under{state space} $S$, any non-empty finite or countable set.
    	\item \under{Initial probabilities} $\{v_i\}_{i \in S}$, where $v_i$ is the probability of starting at $i$ (at time 0). (So $v_i \geq 0$ and $\sum_i v_i = 1$)
    	\item \under{Transition probabilities} $\{p_{ij}\}_{i, j\in S}$, where $p_{ij}$ is the probability of jumping to $j$ if you start at $i$. (So $p_{ij} \geq 0$, and $\sum_j p_{ij} = 1$ for all $i$)
    \end{itemize}
    
    \remark[Markov property]
    $$P(X_{n+1} = j | X_0 = i_0, X_1 = i_1, X_2 = i_2, \hdots, X_n = i_n) = P(X_{n+1} = j | X_n = i_n) = p_{i_nj}$$
    i.e. The probabilities at time $n+1$ depend only on the state at time $n$.
    
    \remark
    $$P(X_0 = i_0, X_1 = i_1, \hdots, X_n = i_n) = v_{i_0}p_{i_0i_1}p_{i_1i_2} \hdots p_{i_{n-1}i_n}$$
    
    \subsection{Markov Chain examples}
    \example[the Frog Walk]
    Let $X_n :=$ pad index the frog is at after $n$ steps.
    \begin{align*}
    	S &= \{1, 2, 3, \hdots, 20\}\\
    	v_{20} &= 1, v_i = 0 \, \forall i \neq 20 \\
    	p_{ij} &= \begin{cases}
    		\frac{1}{3}, \quad &|j - i| \leq 1 \text{ or } |j - i| = 19\\
    	0, \quad &\text{ otherwise }
    	\end{cases}
    \end{align*}
    
    \example[Bernoulli process]
    \begin{align*}
    	S &= \{1, 2, 3, \hdots \}\\
    	v_{0} &= 1, v_i = 0 \, \forall i \neq 0 \\
    	p_{ij} &= \begin{cases}
    		p, \quad  &j = i + 1\\
    		1 - p, \quad &j = i \\
    		0, \quad &\text{ otherwise }
    	\end{cases}
    \end{align*}
    where $0 < p < 1$.
    
    \example[Simple random walk (s.r.w.)]
    Let $X_n :=$ net gain (in dollars) after $n$ bets
    \begin{align*}
    	S &= \{0, 1, 2, 3, \hdots \}\\
    	v_{a} &= 1, v_i = 0 \, \forall i \neq a \\
    	p_{ij} &= \begin{cases}
    		p, \quad  &j = i + 1\\
    		1 - p, \quad &j = i - 1 \\
    		0, \quad &\text{ otherwise }
    	\end{cases}
    \end{align*}
    where $0 < p < 1, a \in \mb{Z}$. \\
    \tb{Special case:} When $p = 1/2$, call it \under{simple symmetric random walk}.
    
    \example[Ehrenfest's Urn]
    Let $X_n:= \#$ balls in Urn 1 at time $n$.\\
    We have $d$ balls in total, divided into two urns. At each time, we choose one of the $d$ balls uniformly at random, and move it to the other urn.\\ 
    \begin{align*}
    	S &= \{1, 2, 3, \hdots, d \}\\
    	v_{a} &= 1, v_i = 0 \, \forall i \neq a \\
    	p_{ij} &= \begin{cases}
    		(d - i) / d, \quad  &j = i + 1\\
    		i / d, \quad &j = i - 1 \\
    		0, \quad &\text{ otherwise }
    	\end{cases}
    \end{align*}
    
    \subsection{Elementary Computations}
    \notation
     $$\mu_i^{(n)} := P(X_n = i)$$ 
    \notation
    \begin{align*}
    	m &:= |S| \tag{the number of elements in S, could be infinity} \\
   		\mu^{(n)} &= (\mu_1^{(n)}, \mu_2^{(n)}, \mu_3^{(n)}, \hdots) \tag{$m \times 1$}\\
   		v &= (v_1, v_2, v_3, \hdots) \tag{$m \times 1$}\\
   		P &= (p_{ij}) = \begin{pmatrix}
   			p_{11} & p_{12} & \hdots & p_{1m}\\
   			p_{21} & p_{22} & \hdots & \\
   			& \ddots & &\\
   			p_{m1} & \hdots & & p_{mm}
   		\end{pmatrix} \tag{$m \times m$ matrix} \\
    \end{align*}
    
    \fact
    \begin{align*}
    	\mu^{(1)} &= vP = \ur{\mu}{0}P \\
    	\mu^{(n)} &= vP^n = \ur{\mu}{0}P^n \\
    \end{align*}
    
    \notation
    \begin{equation}
    	\dur{p}{ij}{n} := P(X_n = j, X_0 = i) = P(X_{m+n} = j | X_m = i) \tag{for any $m \in \mb{N}$}    	
    \end{equation}
    
    \fact
    \begin{align*}
    	\sum_{j\in S} \dur{p}{ij}{n} &= 1 \\
    	\dur{p}{ij}{1} &= p_{ij} \\
    	\ur{P}{n} &= P^n \tag{for all $n \in \mb{N}$}
    \end{align*}
    
    \notation
    \begin{align*}
    	 P^0 &:= I \\
    	 \ur{P}{0} &:= I \\
    	 \dur{p}{ij}{0} &= \begin{cases}
    	 	1 & i = j\\
    	 	0 & \text{otherwise}
    	 \end{cases}
    \end{align*}
    
    

    
    \theorem[Chapman-Kolmogorov equations] 
    \begin{align*}
    	p_{ij}^{(m+n)} &= \sum_{k \in S} p_{ik}^{(m)}p_{kj}^{(n)} \\
    	P_{ij}^{(m + s +n)} &= \sum_{k \in S}\sum_{l \in S} p_{ik}^{(m)} p_{kl}^{(s)}p_{lj}^{(n)}
    \end{align*}
    Matrix form:
	\begin{align*}
		P^{(m + n)} &= P^{(m)}P^{(n)} \\
		P^{(m + s +n)} &= P^{(m)}P^{(s)}P^{(n)}
	\end{align*}
    
    \theorem[Chapman-Kolmogorov Inequality]
    \begin{align*}
    	p_{ij}^{(m+n)} &\geq p_{ik}^{(m)}p_{kj}^{(n)} \tag{for all $k \in S$} \\
    	P_{ij}^{(m + s +n)} &\geq  p_{ik}^{(m)} p_{kl}^{(s)}p_{lj}^{(n)} \tag{for any $k, l \in S$}
    \end{align*}
    
    \subsection{Recurrence and Transience}
    \notation
    \begin{align*}
    	P_i(\hdots) &\equiv P(\hdots | X_0 = i) \\
    	E_i(\hdots) &\equiv E(\hdots | X_0 = i) \\
    	N(i) &= \#\{n \geq 1: X_n = i\} \tag{total number of times that the chain hits $i$, not counting time 0} \\
    \end{align*}
    \definition[\red{return probability}]
    Let $f_{ij}$ be the \under{return probability} from $i$ to $j$.
    \begin{align*}
    	 f_{ij} := P_i(X_n = j \text{ for some } n \geq 1) \equiv P_i(N(j) \geq 1)
    \end{align*}
    
    \fact
    \begin{align}
    	1 - f_{ij} &= P_i(X_n \neq j \text{ for all } n \geq 1) \\
    	P_i(N(i) \geq k) &= (f_{ii})^k \\
    	P_i(N(j) \geq k) &= f_{ij}(f_{jj})^{k-1} \\
    	f_{ik} &\geq f_{ij}f_{jk}
    \end{align}
    
    \fact
    $f_{ij} > 0$ iff $\exists m \geq 1$ with $\dur{p}{ij}{m} > 0$, i.e., there is some time $m$ for which it is possible to get from $i$ to $j$ in $m$ steps.
    
    \definition[\red{recurrent and transient states}]
    A state $i$ of a Markov chain is \under{recurrent} if $f_{ii} = 1$. Otherwise, $i$ is \under{transient} if $f_{ii} < 1$.
    
    \proposition
    If $Z$ is a non-negative integer, then
    $$E(Z) = \sum_{k=1}^\infty P(Z \geq k)$$
    
    \theorem[\red{Recurrent State Theorem}] As follows
    \begin{itemize}
    	\item  State $i$ is recurrent $\iff P_i(N(i) = \infty) = 1 \iff \sum_{n=1}^\infty p_{ii}^{(n)} = \infty$
    	\item State $i$ is transient $\iff P_i(N(i) = \infty) = 0 \iff \sum_{n=1}^\infty p_{ii}^{(n)} < \infty$ 
    \end{itemize}
    \begin{proof}
    	\begin{align*}
    		P_i(N(i) = \infty) &= \limit{k} P_i(N(i) \geq k) \tag{by continuity of probabilities} \\
    		&= \limit{k} (f_{ii})^k \tag{$P_i(N(i) \geq k) = (f_{ii})^k$}\\
    		&= \begin{cases}
    			1, & f_{ii} = 1 \\
    			0, & f_{ii} < 1
    		\end{cases}
    	\end{align*}
    \end{proof}
    Therefore,
    \begin{align*}
    	\sum_{n=1}^\infty \dur{p}{ii}{n} &= \sum_{n=1}^\infty P_i(X_n = i) \\ 
    	&= \sum_{n=1}^\infty E_i(\id{X_n = i}) \\
    	&= E_i(\sum_{n=1}^\infty \id{X_n = i}) \\
    	&= E_i(N(i)) \\
    	&= \sum_{k=1}^\infty P_i(N(i) \geq k) \tag{by proposition 1.1}\\
    	&= \sum_{k=1}^\infty (f_{ii})^k \\
    	&= \begin{cases}
    		\infty, & f_{ii} = 1\\
    		\frac{f_{ii}}{1 - f_{ii}} < \infty, & f_{ii} < 1
    	\end{cases}
    \end{align*}
    
    \example[simple random walk]
    If $p = 1/2$ then $\forall i, f_{ii} = 1$. If $p \neq 1/2$, then $\forall i, f_{ii} < 1$ 
    \begin{proof}
   	Consider state 0. We need to check if $\sum_{n=1}^\infty \dur{p}{00}{n} = \infty$. \\
   	If $n$ is odd, then $\dur{p}{00}{n} = 0$.\\
   	If $n$ is even, $\dur{p}{00}{n} = P(\frac{n}{2} \text{ heads and } \frac{n}{2} \text{ tails on first } n \text{ tosses})$.\\ This is a Binomial$(n,p)$ distribution, so
   	\begin{align*}
   		\dur{p}{00}{n} &= {n \choose n/2} p^{n/2}(1-p)^{n/2} \\
   					   &= \frac{n!}{[(n/2)!]^2}p^{n/2}(1-p)^{n/2} \\
   					   &= \frac{(n/e)^n\sqrt{2\pi n}}{[(n/2e)^{n/2}\sqrt{2\pi n/ 2}]^2}p^{n/2}(1-p)^{n/2} \tag{Sirling's approximation}\\
   					   &= [4p(1-p)]^{n/2}\sqrt{2/\pi n} 
   	\end{align*}
   	\tb{Case 1:} If $p = 1/2$, then $4p(1-p) = 1$, so
   	\begin{align*}
   		\sum_{n=1}\infty \dur{p}{00}{n} &= \sum_{n=2,4,6,\hdots} \sqrt{2/\pi n} \\
   		&= \sqrt{2/\pi}\sum_{n=2,4,6,\hdots} n^{-1/2} \\
   		&= \sqrt{2/\pi}\sum_{n=1}^\infty 2k^{-1/2} \\
   		&= \infty
   	\end{align*} 
   	Therefore, state 0 is recurrent. \\
   	\tb{Case 2:} If $p \neq 1/2$, then $4p(1-p) < 1$, so
   	\begin{align*}
   		\sum_{n=1}\infty \dur{p}{00}{n} &= \sum_{n=2,4,6,\hdots} [4p(1-p)]^{n/2}\sqrt{2/\pi n} \\
   		&< \sum_{n=2,4,6,\hdots} [4p(1-p)]^{n/2} \tag{Geometric Series} \\
   		&= \frac{4p(1-p)}{1-4p(1-p)} \\
   		&< \infty
   	\end{align*} 
   	Therefore, the state 0 is transient.\\
   	The same exact calculation applies to any other state $i$.
    \end{proof}
    
    \theorem[f-Expansion]
    $$f_{ij} = p_{ij} + \sum_{k \in S, k \neq j} p_{ik}f_{kj}$$
    \begin{proof}
    	\begin{align*}
    		f_{ij} &= P_i(\exists n \geq 1: X_n = j) \\
    		&= \sum_{k \in S} P_i(X_1 = k, \exists n \geq 1: X_n = j) \\
    		&= P_i(X_1 = j, \exists n \geq 1: X_n = j) + \sum_{k \neq j} P_i(X_1 = k, \exists n \geq 1: X_n = j) \\
    		&= P_i(X_1 = j)P_i(\exists n \geq 1: X_n = j | X_1 = j) + \sum_{k \neq j} P_i(X_1 = k)P_i( \exists n \geq 1: X_n = j | X_1 = k) \\
    		&= p_{ij}(1) + \sum_{k \neq j}p_{ik}(f_{kj})
    	\end{align*}
    \end{proof}
    
    \remark
    The f-Expansion shows that $f_{ij} \geq p_{ij}$.
    
    \remark
    It essentially follows from logical reasoning: from $i$, to get to $j$ eventually, we have to either jump to $j$ immediately (with probability $p_{ij}$), or jump to some other state $k$ (with probability $p_{ik}$) and then get to $j$ eventually (with probability $p_{kj}$)
    \subsection{Communicating States and Irreducibility}
    \definition[communicating states] State $i$ \under{communicates} with state $j$, written $i \rightarrow j$, if $f_{ij} > 0$.
    \remark
    i.e. if it is possible to get from $i$ to $j$.
    \notation
    Write $i \leftrightarrow j$ if both $i \rightarrow j$ and $j \rightarrow i$.
    
    \definition[irreducibility] A Markov chain is \under{irreducible} if $i \rightarrow j$ for all $i, j \in S$, i.e., if $f_{ij} > 0$ for all $i, j \in S$. Otherwise, the chain is \under{reducible}.
    
    \lemma[Sum Lemma]
    If $i \rightarrow k$, and $l \rightarrow j$, and $\sum_{n=1}^\infty p_{kl}^{(n)} = \infty$, then $\sum_{n=1}^\infty p_{ij}^{(n)} = \infty$
    \begin{proof}
    	Since $i \rightarrow k$, and $l \rightarrow j$, there exists $m, r \geq 1$ s.t. $\dur{p}{ik}{m} > 0$ and $\dur{p}{lj}{r} > 0$. \\
    	By the Chapman-Kolmogorov inequality, $$\dur{p}{ij}{m+s+r} \geq \dur{p}{ij}{m}\dur{p}{kl}{s}\dur{p}{lj}{r}$$
    	Hence
    	\begin{align*}
    		\sum_{n=1}^{\infty} \dur{p}{ij}{n} &\geq \sum_{n = m + r + 1}^{\infty} \dur{p}{ij}{n} \\
    		&= \sum_{s=1}^\infty \dur{p}{ij}{m + s + r} \tag{$s = n - m -r$}\\
    		&\geq \sum_{s=1}^\infty \dur{p}{ij}{m}\dur{p}{kl}{s}\dur{p}{lj}{r} \\
    		&= \underbrace{\dur{p}{ij}{m}}_{+}\underbrace{\dur{p}{lj}{r}}_{+}\underbrace{\sum_{s=1}^{\infty}\dur{p}{kl}{s}}_{=\infty}\\
    		&= \infty
    	\end{align*}

    \end{proof}
    \corollary[Sum Corollary] If \blue{$i \leftrightarrow k$}, then $i$ is recurrent iff $k$ is recurrent.
    \begin{proof}
    	Setting $j = i$ and $l = k$ in the Sum Lemma: If $i \leftrightarrow k$, then $\sum_{n=1}^\infty \dur{p}{ii}{n} = \infty \iff \sum_{n=1}^\infty \dur{p}{kk}{n} = \infty$.
    \end{proof}
    
    \theorem[Cases Theorem]
    For an \blue{irreducible} Markov chain, either
    \begin{itemize}
    	\item (a) $\sum_{n=1}^\infty p_{ij}^{(n)} = \infty$ for all $i, j \in S$, and all states are recurrent (\under{recurrent Markov chain}); or
    	\item (b) $\sum_{n=1}^\infty p_{ij}^{(n)} < \infty$ for all $i, j \in S$, and all states are transient (\under{transient Markov chain}).
    \end{itemize}
    
    \theorem[Finite Space Theorem] An irreducible Markov chain on a \blue{finite} state space always falls into case (a), i.e., $\sum_{n=1}^\infty p_{ij}^{(n)} = \infty$ for all $i, j \in S$, and all states are recurrent.
    \begin{proof}
    	Choose any state $i \in S$. We have
    	\begin{align*}
    		\sum_{j \in S}\sum_{n=1}^\infty \dur{p}{ij}{n} &= \sum_{n=1}^\infty\sum_{j \in S} \dur{p}{ij}{n} \tag{exchanging the sums} \\
    		& = \sum_{n=1}^\infty 1 \\
    		&= \infty
    	\end{align*}
    	Then if $S$ is finite, it follows that there must exist at least one $j \in S$ with $\sum_{n=1}^\infty p_{ij}^{(n)} = \infty$. So we must be in case (a).
    	
    \end{proof}
    \notation
    For $i \neq j$, let $H_{ij}$ be the event that the chain hits the state $i$ before returning to $j$, i.e.,
    $$H_{ij} = \{ \exists n \in \mb{N}: X_n = i, \text{ but } X_m \neq j \text{ for } 1 \leq m \leq n - 1\}$$
    \lemma[Hit Lemma] If \blue{$j \rightarrow i$ with $j \neq i$}, then $P_j(H_{ij}) > 0$.
    
    \begin{proof}
    	Since $j \rightarrow i$, there is some possible path from $j$ to $i$. i.e., there is $m \in \mb{N}$ and $x_0, x_1, \hdots, x_m$ with $x_0 = j$ and $x_m = i$ and $p_{x_rx_{r+1}} > 0$ for all $0 \leq r \leq m - 1$. \\
    	Let $S = \max\{r: x_r = j\}$ be the last time this path hits $j$. \\
    	Then $x_S, x_{S+1}, \hdots, x_m$ is a possible path which goes from $j$ to $i$ without first returning to $j$. \\
    	Hence $P_j(H_{ij}) \geq P(x_0, x_1, \hdots, x_m) = p_{x_Sx_{S+1}}p_{x_{S+1}x_{S+2}}\hdots p_{x_{m-1}x_m} > 0$
    	
    	
    \end{proof}
  
  	\remark
  	If it is possible to get from $j$ to $i$ at all, then it is possible to get from $j$ to $i$ without first returning to $j$. \\
  	Intuitively obvious: If there is some path from $j$ to $i$, then the final part of the path (starting with the last time it visits $i$) is a possible path from $j$ to $i$ which does not return to $j$.
  	\lemma[f-Lemma] If \blue{$j \rightarrow i$ and $f_{jj} = 1$}, then $f_{ij} = 1$
  	\begin{proof}
  		If $i = j$ it is trivial, so assume $i \neq j$. \\
  		Since $j \rightarrow i$, we have $P_j(H_{ij}) > 0$ by the Hit Lemma.\\
  		But one way to never return to $j$ is to first hit $i$ and then from $i$ never return to $j$:
  		$$P_j(\text{never return to $j$}) \geq P_j(H_{ij})P_i(\text{never return to $j$})$$
  		Therefore
  		$$1 - f_{jj} \geq P_j(H_{ij})(1 - f_{ij})$$
  		Since $f_{jj} = 1$, then $\underbrace{P_j(H_{ij})}_{>0}(1 - f_{ij}) = 0$\\
  		Hence $f_{ij} = 1$.
  		
  	\end{proof}
    
    \lemma[Infinite Returns Lemma] For an \blue{irreducible} Markov chain, if it is \blue{recurrent}, then $$P_i(N(j) = \infty) = 1$$ for all $i, j \in S$. \\
    But if it \blue{transient}, then $P_i(N(j) = \infty) = 0$ for all $i, j \in S$.
    \begin{proof}
    	Let $i, j \in S$. If the chain is recurrent, then $f_{ij} = f_{jj} = 1$ by the f-Lemma.\\
    	Then
    	\begin{align*}
    		P_i(N(j) = \infty) &= \limit{k}P_i(N(j) \geq k) \\
    		&= \limit{k} f_{ij}(f_{jj})^{k-1} \\
    		&= \limit{k} (1)(1)^{k-1} \\
    		&= 1
    	\end{align*}
    	If the chain is transient, then $f_{jj} < 1$, then
    	 \begin{align*}
    		P_i(N(j) = \infty) &= \limit{k}P_i(N(j) \geq k) \\
    		&= \limit{k} f_{ij}(f_{jj})^{k-1} \\
    		&= \limit{k} (1)(f_{jj})^{k-1} \\
    		&= 0
    	\end{align*}
    	
    \end{proof}
    
    
    \theorem[\red{Recurrence Equivalence Theorem}]
    If a chain is \blue{irreducible}, then the following are equivalent (and all correspond to case (a)):
    \begin{enumerate}
    	\item There are $k, l \in S$ with $\sum_{n=1}^\infty p_{kl}^{(n)} = \infty$.
    	\item For all $i, j \in S$, we have $\sum_{n=1}^\infty p_{ij}^{(n)} = \infty$.
    	\item There is $k \in S$ with $f_{kk} = 1$, i.e. $k$ is recurrent.
    	\item For all $j \in S$, we have $f_{jj} = 1$, i.e. all states are recurrent.
    	\item For all $i, j \in S$, we have $f_{ij} = 1$.
    	\item There are $k, l \in S$ with $P_k(N(l) = \infty) = 1$.
    	\item For all $i, j \in S$, we have $P_i(N(j) = \infty) = 1$.
    \end{enumerate}
    \begin{proof}
    Follow from results that we have already proven
    	\begin{itemize}
    		\item $1 \implies 2$: Sum Lemma.
    		\item $2 \implies 4$: Recurrent State Theorem (with $i = j$).
    		\item $4 \implies 5$: f-Lemma.
    		\item $5 \implies 3$: immediate.
    		\item $3 \implies 1$: Recurrent State Theorem (with $l = k$).
    		\item $4 \implies 7$: Infinite Returns Lemma.
    		\item $7 \implies 6$: Immediate.
    		\item $6 \implies 3$: Recurrent State Theorem (with $l = k$).
    	\end{itemize}
    	
    	
    \end{proof}
    \theorem[\red{Transience Equivalence Theorem}]
    If a chain is \blue{irreducible}, then the following are equivalent (and all correspond to case (b)):
    \begin{enumerate}
    	\item There are $k, l \in S$ with $\sum_{n=1}^\infty p_{kl}^{(n)} < \infty$.
    	\item For all $i, j \in S$, we have $\sum_{n=1}^\infty p_{ij}^{(n)} < \infty$.
    	\item For all $k \in S$, we have $f_{kk} < 1$, i.e. $k$ is transient.
    	\item There is $j \in S$ with $f_{jj} < 1$, i.e. some state is recurrent.
    	\item There are $i, j \in S$ with $f_{ij} < 1$.
    	\item For all $k, l \in S, P_k(N(l) = \infty) = 0$.
    	\item There are $i, j \in S$ with $P_i(N(j) = \infty) = 0$.
    \end{enumerate}    
    \remark[closed subset note]
    Suppose a chain is reducible, but it has a closed subset $C \subseteq S$ (i.e. $p_{ij} = 0$ for $i \in C$ and $j \notin C$) on which it is irreducible (i.e. $i \rightarrow j$ for all $i, j \in C$). Then, the Recurrence Equivalence Theorem and other results about irreducible chains still apply to the chain when \blue{restricted} to $C$.
    
    \proposition
    For simple random walk with $p > 1/2, f_{ij} = 1$ whenever $j > i$. (Similarly, if $p < 1/2$ and $j < i$, then $f_{ij} = 1$.)
    \begin{proof}
    	Let $X_0 = 0$, and $Z_n = X_n -X_{n-1}$ for $n = 1, 2, \hdots$, so that $X_n = \sum_{i=1}^n Z_i$. \\
    	Since $Z_n$s iid with $P(Z_n = 1) = p$ and $P(Z_n = -1) = 1- p$, then by Law of Large Numbers,
    	$$\limit{n} \frac{1}{n}(Z_1 + Z_2 + \hdots + Z_n) \overset{p}{=} E(Z_1) = p(1) + (1 - p)(-1) = 2p - 1 > 0$$
    	\begin{align*}
    		\implies \infty &= \limit{n}(Z_1 + Z_2 + \hdots + Z_n)\\
    		 &= \limit{n} X_n - X_0 \\
    		 &= \limit{n} X_n
    	\end{align*}
    	But if $i < j$, then to go from $i$ to $\infty$, the chain must pass through $j$, so $f_{ij} = 1$.
    \end{proof}
    
    \section{Markov Chain Convergence}
    \subsection{Stationary Distributions}
    \definition[stationary distributions]
    If $\pi$ is a probability distribution on $S$ (i.e. $\pi_i \geq 0$ for all $i \in S$, and $\sum_{i \in S} \pi_i = 1$), then $\pi$ is \under{stationary} for a Markov chain with transition probabilities $(p_{ij})$ if $\sum_{i \in S} \pi_ip_{ij} = \pi_j$ for all $j \in S$ (or $\pi P = \pi$, in matrix notation).
    \remark
    Intuitively, $\pi$ being stationary means if the chain starts with probabilities $\{\pi_i\}$, then it will keep the same probabilities one time unit later.
    
    \definition[doubly stochastic]
    A Markov Chain is \under{doubly stochastic} if in addition to the usual condition that $\sum_{j \in S}p_{ij} = 1$ for all $i \in S$, $\sum_{i \in S}p_{ij} = 1$ for all $j \in S$.
    \remark
    This holds for the Frog Example.
    
    \proposition
    If a Markov chain with states $S$ satisfies \blue{$|S| < \infty$ and is doubly stochastic}, then the uniform distribution on $S$ is a stationary distribution.
    \begin{proof}
    	Let $\{\pi_i\}$ be a distribution such that $\pi_i = \frac{1}{|S|}$. \\
    	Then
    	\begin{align*}
    		\sum_{i \in S}\pi_i p_{ij} &= \sum_{i \in S} \frac{1}{|S|}p_{ij} \\
    		&= \frac{1}{|S|}\sum_{i \in S} p_{ij} \\
    		&= \frac{1}{|S|}(1) \tag{doubly stochastic}\\
    		&= \frac{1}{|S|} \\
    		&= \pi_j
    	\end{align*}
    Then $\{\pi_i\}$ is stationary.
    \end{proof}
    
    \subsection{Searching for Stationary}
    \definition[reversibility]
    A Markov chain is \under{reversible} (or time reversible, or satisfies detailed balance) with respect to a probability distribution $\{\pi_i\}$ if $\pi_ip_{ij} = \pi_jp_{ji}$ for all $i, j \in S$.
    
    \proposition If a chain is reversible with respect to $\pi$, then $\pi$ is a stationary distribution.
    \begin{proof}
    	Reversibility means $\pi_ip_{ij} = \pi_jp_{ji}$, so then for $j \in S$, $$\sum_{i \in S}\pi_ip_{ij} = \sum_{i\in S}\pi_jp_{ji} = \pi_j\sum_{i\in S}p_{ji} = \pi_j(1) = \pi_j$$
    \end{proof}
    \lemma[M-test]
    Let $\{x_{nk}\}_{n, k\in \mb{N}}$ be a collection of real numbers. Suppose that $\limit{n} x_{nk}$ exists for each fixed $k \in \mb{N}$. Suppose further that $\sum_{k=1}^\infty \underset{n}{\sup} \,|x_{nk}| < \infty$. Then $\limit{n}\sum_{k=1}^\infty x_{nk} = \sum_{k=1}^\infty\limit{n}x_{nk}$.
    \proposition[Vanishing Probabilities Proposition]
    If a Markov chain's transition probabilities satisfy that \blue{$\underset{n \rightarrow \infty}{\lim} p_{ij}^{(n)} = 0$ for all $i, j \in S$}, then the chain does \red{not} have a stationary distribution.
    \begin{proof}
    	Suppose for contradiction that there is a stationary distribution $\pi$. Then we would have $\pi_j = \sum_{i \in S} \pi_i\dur{p}{ij}{n}$ for any $n$, so
    	$$\pi_j = \limit{n}\pi_j = \limit{n}\sum_{i \in S}\pi_i \dur{p}{ij}{n}$$
    	\begin{align*}
    		\pi_j &= \limit{n}\pi_j \\
    			  &= \limit{n}\sum_{i \in S}\pi_i \dur{p}{ij}{n} \\
    			  &= \sum_{i \in S}\limit{n}\pi_i \dur{p}{ij}{n} \tag{exchange the sum and the limit, which is valid by M-test}\\
    			  &= \sum_{i \in S}\pi_i\limit{n} \dur{p}{ij}{n} \\
    			  &=	 \sum_{i \in S} 0 \\
    			  &= 0
    	\end{align*}
    	So we would have $\pi_j = 0$ for all $j$. But this means that $\sum_j \pi_j = 0$, which is a contradiction.\\
    	
    \end{proof}
    \lemma[Vanishing Lemma] If a Markov chain has some $k, l \in S$ with $\limit{n}{p_{kl}^{(n)}} = 0$, then for any $i, j \in S$ with $k \rightarrow i$ and $j \rightarrow l$, $\limit{n}{p_{ij}^{(n)}}= 0$.
    \begin{proof}
    Since $k\rightarrow i$ and $j \rightarrow l$, we can find $r, s\in\mb{N}$ with $\dur{p}{ki}{r} > 0$ and $\dur{p}{jl}{s} > 0$. Then by the Chapman-Kolmogorov Inequality,
    $$\dur{p}{kl}{r+n+s} \geq \dur{p}{ki}{r}\dur{p}{ij}{n}\dur{p}{jl}{s}$$
    Hence
    $$\dur{p}{ij}{n} \leq \dur{p}{kl}{r+n+s}/\dur{p}{ki}{r}\dur{p}{jl}{s}$$
    But the assumptions imply that 
    $$\limit{n} \left[ \dur{p}{kl}{r+n+s}/\dur{p}{ki}{r}\dur{p}{jl}{s} \right] =0$$
    Hence
    $$0 \leq \limit{n}{p_{ij}^{(n)}} \leq 0$$
    $$\implies \limit{n}{p_{ij}^{(n)}}= 0$$
    	
    \end{proof}
    
    \corollary[Vanishing Together Corollary] For an \blue{irreducible} Markov chain, either
    \begin{enumerate}
    	\item $\limit{n} p_{ij}^{(n)} = 0$ for all $i, j \in S$, or
    	\item $\limit{n} p_{ij}^{(n)} \neq 0$ for all $i, j \in S$
    \end{enumerate}
    
	\corollary[Vanishing Probabilities Corollary] If an \blue{irreducible} Markov chain's transition probabilities satisfy that $\limit{n} p_{kl}^{(n)} = 0$ for some $k, l 
	\in S$, then the chain does not have a stationary distribution.  
    
    \lemma If the $x_n$s are non-negative, and $\sum_{n=1}^\infty x_n < \infty$, then $\limit{n} x_n = 0$.
    \corollary[Transient Not Stationary Corollary] A Markov chain which is \blue{irreducible and transient} cannot have a stationary distribution.
    \begin{proof}
    	If a chain is irreducible and transient, then by the Transience Equivalence Theorem,
    	$\sum_{n=1}^\infty < \infty$ for all $i, j \in S$. Hence $\limit{n}\dur{p}{ij}{n} = 0$ for all $i, j \in S$. \\
    	Thus by the Vanishing Probabilities Corollary, there is no stationary distribution.
    	
    	
    	
    \end{proof}
    \subsection{Obstacles to Convergence}
    \definition[period] The \under{period} of a state $i$ is the greatest common divisor (gcd) of the set $\{n \geq 1: p_{ii}^{(n)} > 0\}$, i.e. the largest number $m$ such that all the values of $n$ with $p_{ii}^{(n)} > 0$ are all integer multiples of $m$. If the period of each state is 1, we say the chain is \under{aperiodic}; otherwise we say the chain is \under{periodic}.
    
    \remark
    Intuitively, the period of a state $i$ is the pattern of returning to $i$ from $i$. e.g. If the period of $i$ is 2, then it is only possible to get from $i$ to $i$ in an even numbers of steps.
    
    \fact
    If state $i$ has period $t$, and $\dur{p}{ii}{m} > 0$, then $m$ is an integer multiple of $t$, i.e., $t$ divides $m$.
    
    \fact
    If $p_{ii} >0$, then the period of state $i$ is 1.
    
    \fact
    If $\dur{p}{ii}{n} > 0$ and $\dur{p}{ii}{n+1} > 0$, then the period of state $i$ is 1.
    
    \lemma[Equal Periods Lemma] If $i \leftrightarrow j$, then the periods of $i$ and of $j$ are equal.
    \begin{proof}
    	Let the periods of $i$ and $j$ be $t_i$ and $t_j$. Since $i \leftrightarrow j$, we can find $r, s\in \mb{N}$ with $\dur{p}{ij}{r} > 0$ and $\dur{p}{ji}{s} > 0$. Then
    	$$\dur{p}{ii}{r+s} \geq \dur{p}{ij}{r}\dur{p}{ji}{s} > 0$$
    	Therefore by Fact 2.1, $t_i$ divides $r + s$.\\
    	Suppose now that $\dur{p}{jj}{n} > 0$. Then
    	$$\dur{p}{ii}{r + n +s} \geq \dur{p}{ij}{r}\dur{p}{jj}{n} \dur{p}{ji}{s} > 0$$
    	So $t_i$ divides $r + n +s$. \\
    	Since $t_i$ divides both $r + n +s$ and $r + s$, then it must divide $n$ as well. \\
    	Since this is true for any $n$ with $\dur{p}{jj}{n} > 0$, it follows that $t_i$ is a common divisor of $\{n \in \mb{N}: \dur{p}{jj}{n} > 0\}$.\\
    	But $t_j$ is the \blue{greatest} such common divisor, so $t_j \geq t_i$.\\
    	Similarly we can show that $t_i \geq t_j$, so we have $t_i = t_j$.
  
    	
    \end{proof} 
    
    \corollary[Equal Periods Corollary]If a chain is \blue{irreducible}, then all states have the same period.
    
    \corollary If a chain is \blue{irreducible and $p_{ii} > 0$ for some state $i$}, then the chain is \red{aperiodic}.
    
    \subsection{Convergence Theorem}
    \theorem[Markov Chain Convergence Theorem] If a Markov chain is \blue{irreducible, aperiodic, and has a stationary distribution $\{\pi_i\}$}, then $\limit{n} p_{ij}^{(n)} = \pi_j$ for all $i, j \in S$, and $\limit{n} P(X_n = j) = \pi_j$ for any initial probabilities $\{v_i\}$.
    
    \theorem[Stationary Recurrence Theorem] If chain \blue{irreducible and has a stationary distribution}, then it is \red{recurrent}.
    \begin{proof}
    	The Transient Not Stationary Corollary says that a chain cannot be irreducible, transient and have a stationary distribution. \\
    	Therefore, if a chain is irreducible and has a stationary distribution, then it cannot be transient, i.e. it must be recurrent.
    \end{proof}
    
    \lemma[Number Theory Lemma] If a set $A$ of positive integers is non-empty, and satisfies additivity, and $gcd(A) = 1$, then there is some $n_0 \in \mb{N}$ s.t. for all $n \geq n_0$ we have $n \in A$ i.e. the set $A$ includes all of the integers $n_0, n_0 + 1, n_0 + 2, \hdots$
    \proposition If a state $i$ \blue{has $f_{ii} > 0$ and is aperiodic}, then there is $n_0(i) \in \mb{N}$ such that $p_{ii}^{(n)} > 0$ for all $n \geq n_0(i)$
    \begin{proof}
    	Let $A = \{ n \geq 1: \dur{p}{ii}{n} > 0\}$. Since $f_{ii} > 0$, then $A$ is not empty.\\
    	If $m, n \in A$, then $$\dur{p}{ii}{m+n} \geq \dur{p}{ii}{m} \dur{p}{ii}{n} > 0$$
    	So $m+n \in A$, which shows that $A$ satisfies additivity. Also $gcd(A) = 1$ since the state $i$ is aperiodic. Hence from the Number Theory Lemma, there is $n_0 \in \mb{N}$ such that for all $n \geq n_0$, we have $n \in A$ i.e. $\dur{p}{ii}{n} > 0$.
    	
    \end{proof}
    
    \corollary If a chain is \blue{irreducible and aperiodic}, then for any states $i, j \in S$, there is $n_0(i,j) \in \mb{N}$ s.t. $p_{ij}^{(n)} > 0$ for all $n \geq n_0(i, j)$
    \begin{proof}
    	Find $n_0(i)$ as in Proposition 2.3, and find $m \in \mb{N}$ with $p_{ij}^{(m)} > 0$.\\
    	Then let $n_0(i,j) = n_0(i) + m$ \\
    	Then if $n \geq n_0(i,j)$, then $n - m \geq n_0(i)$, so $\dur{p}{ij}{n} \geq \dur{p}{ii}{n-m}\dur{p}{ij}{m} > 0$.
    \end{proof}
    
    \lemma[Markov Forgetting Lemma] If a Markov chain is \blue{irreducible and aperiodic, and has stationary distribution $\{\pi_i\}$}, then for all $i, j, k \in S$, 
    $$\limit{n} \left| \dur{p}{ik}{n} - \dur{p}{jk}{n} \right | = 0$$
    \remark
    Intuitively, after a long time $n$, the chain ``forgets" whether it started from state $i$ or from state $j$.
    \begin{proof}
    \todo{long}
    \end{proof}
    \paragraph{Proof of Markov Chain Convergence Theorem}
    \todo{long}
    \corollary If a chain is \blue{irreducible}, then it has at most \red{one} stationary distribution.
    \begin{proof}
    	By Markov Chain Convergence Theorem, any stationary distribution that ie has must be equal to $\limit{n} P(X_n = j)$, so it is unique.
    \end{proof}
    
    \definition[convergence in distribution]
    $$\forall a < b, \underset{n \rightarrow \infty} P(a < X_n < b) = P(a < X < b)$$

    \definition[weak convergence]
    $$\forall \epsilon > 0, \underset{n \rightarrow \infty}{\lim} P(|X_n - X| \geq \epsilon) = 0$$
    \remark This is ``converge in probability".
    
    \definition[strong convergence]
    $$P(\underset{n \rightarrow \infty}{\lim} X_n = X) = 1$$
    \remark This is ``converge almost surely".
    \remark Strong convergence implies weak convergence, and weak convergence implies convergence in distribution.
    
    
    \proposition If $\{X_n\}$ is a simple symmetric random walk, then the absolute values $|X_n|$ converge weakly to positive infinity.
    \todo{prove this}
    
    \subsection{Periodic Convergence}
    \theorem[Periodic Convergence Theorem]
    Suppose a Markov chain is \blue{irreducible}, with \blue{period $b \geq 2$}, and \blue{stationary distribution $\{\pi_i\}$}. Then for all $i, j \in S$,
    $$\limit{n} \frac{1}{b}[\dur{p}{ij}{n} + \hdots + \dur{p}{ij}{n + b -1}] = \pi_j$$
    and 
    $$\limit{n} \frac{1}{b}(P[X_n = j] + P[X_{n+1} = j] + \hdots + P[X_{n+b-1} = j]) = \pi_j$$
    and also
    $$\limit{n} \frac{1}{b}P(X_n = j \text{ or } X_{n+1} = j \text{ or } \hdots \text{ or } X_{n+b-1} = j ) = \pi_j$$
    
    \theorem[Average Probability Convergence]
    If a Markov chain is \blue{irreducible} with \blue{stationary distribution $\{\pi_i\}$} (whether periodic or not), then 
    $$\forall i, j \in S, \limit{n} \frac{1}{n}[\dur{p}{ij}{1} + \dur{p}{ij}{2} + \hdots + \dur{p}{ij}{n}] = \pi_j$$
    i.e.,
    $$\limit{n} \frac{1}{n}\sum_{l=1}^n \dur{p}{ij}{l} = \pi_j$$
    \todo{prove this}
    
    \corollary[Unique Stationary Corollary] If Markov chain $P$ is \blue{irreducible} (whether periodic or not), then it has at most \tb{one} stationary distribution.
    
    \subsection{Application - MCMC Algorithms}
    \improvement[inline]{section missing...}
    
    \subsection{Application - Random Walks on Graphs}
    Let $V$ be a non-empty finite or countable set. Let $w: V \times V \rightarrow [0, \infty)$ be a symmetric weight function so that $w(u,v) = w(v,u)$. (usual unweighted case: $w(u, v) = 1$ if there is an edge between $u$ and $v$, otherwise $w(u,v) = 0$).\\
    Let $d(u) = \sum_{v \in V} w(u,v)$ be the \under{degree} of the vertex $u$. Assume that $d(u) > 0$ for all $u \in V$ (for example, by giving any isolated point a self-edge).
    \definition [(simple) random walk on the (undirected) graph]
    Given a vertex set $V$ with symmetric weights $w$, the \under{(simple) random walk on the (undirected) graph} $(V, w)$ is the Markov chain with state space $S = V$ and transition probabilities $p_{uv} = \frac{w(u,v)}{d(u)}$ for all $u, v \in V$.
    
    \remark
    It follows that 
    $$\sum_{v \in V} p_{uv} = \frac{\sum_{v \in V}w(u, v)}{\sum_{v \in V}w(u,v)} = 1$$
    
    \remark
    The most common case is where each $w(u,v) = 0$ or $1$, so from $u$, the chain moves to one of the $d(u)$ vertices connected to $u$ with equal probability.
    
    \theorem[Graph Stationary Distribution]
    Consider a random walk on a graph $V$ with degrees $d(u)$. Assume that $Z$ is \blue{finite}. Then if $\pi_u = \frac{d(u)}{Z}$, then $\pi$ is a stationary distribution for this walk. 
    
    \theorem[Graph Convergence Theorem] For a random walk on a connected non-bipartite graph, if $Z < \infty$, then $\limit{n} \dur{p}{uv}{n} = \frac{d(v)}{Z}$ for all $u, v \in V$, and $\limit{n} P[X_n = v] = \frac{d(v)}{Z}$ (for any initial probabilities).
    \todo{prove this}
    
    \theorem[Graph Average Convergence] For a random walk on any connected graph with $Z < \infty$ (whether bipartite or not), for all $u, v \in V$, 
    $$\limit{n} \frac{1}{2}[\dur{p}{uv}{n} + \dur{p}{uv}{n+1}] = \frac{d(v)}{Z}$$
    and
    $$\limit{n} \frac{1}{n}\sum_{l=1}^n \dur{p}{uv}{l} = \frac{d(v)}{Z}$$
    \todo{prove this}
    
    \subsection{Application - Gambler's Ruin}
    Consider the following gambling game: \\
    Let $0 < a < c$ be integers, and let $0 < p < 1$. Suppose player $A$ starts with $a$ dollars, player $B$ starts with $c - a$ dollars, and they repeatedly bet. At each bet, $A$ wins $\$1$ from $B$ with probability $p$, or $B$ wins $\$ 1$ from $A$ with probability $1 - p$.\\
    If $X_n$ is the amount of money that $A$ has at time $n$, then clearly $X_0 = a$, and $\{X_n\}$ follows a simple random walk.\\
    Let $T_i = \inf\{n \geq 0: X_n = i\}$ be the first time $A$ has $i$ dollars.
    \paragraph{The Gambler's Ruin question} What is $P_a(T_c < T_0)$, i.e., what is the probability that $A$ reaches $c$ dollars before losing all their money? \\
    \ti{Answer: } \blue{Define $s(a) := P_a(T_c < T_0)$}, so that the probability we want to find is a function of the player's initial fortune $a$. Clearly $s(0) = 0$ and $s(c) = 1$.\\
    For $1 \leq a \leq c - 1$, we have
    \begin{align*}
    	s(a) &= P_a(T_c < T_0) \\
    	&= P_a(T_c < T_0, X_0 + 1) + P_a(T_c < T_0, X_1 = X_0 - 1) \tag{$A$ either wins or loses $\$1$ on the first bet} \\
    	&= P(X_1 = X_0 + 1)P_a(T_c < T_0 | X_1 = X_0 + 1)+P(X_1 = X_0 - 1)P_a(T_c < T_0 | X_1 = X_0 - 1)\\
    	&= ps(a+1) + (1-p)s(a-1)
    \end{align*}
    This gives $c - 1$ equations for the $c - 1$ unknowns, which can be solved by simple algebra:
    \begin{align*}
    	ps(a) + (1-p)s(a) &= ps(a+1) + (1-p)s(a-1) \tag{re-arranging}\\
    	\implies s(a+1) - s(a) &= \frac{1-p}{p}[s(a) - s(a-1)]\\
    \end{align*}
    Suppose $s(1) = x$ for some $x \in \real$, then
    \begin{align*}
    	s(1) - s(0) &= x \\
    	s(2) - s(1) &= \frac{1-p}{p}[s(1) - s(0)] = \frac{1-p}{p}x\\
    	s(3) - s(2) &= \frac{1-p}{p}[s(2) - s(1)] = \left(\frac{1-p}{p}\right)^2x \\
    	\implies s(a+1) - s(a) &= \left(\frac{1-p}{p}\right)^ax \quad \tag{for $1 \leq a \leq c$}\\
    	\implies s(a) &= s(a) - s(0) \\
    	&= [s(a) - s(a-1)] + [s(a-1) - s(a-2)] + \hdots + [s(1) - s(0)] \\
    	&= \left[\left(\frac{1-p}{p}\right)^{a-1} + \left(\frac{1-p}{p}\right)^{a-2} + \hdots + \left(\frac{1-p}{p}\right) + 1 \right]x\\
    	&= \begin{cases}
    		\left[ \frac{(\frac{1-p}{p})^a - 1}{(\frac{1-p}{p}) - 1} \right] x, \quad &p \neq \frac{1}{2} \\
    		ax, \quad & p = \frac{1}{2}
    	\end{cases}
    \end{align*}
    Since $s(c) = 1$, we can solve for $x$:
    $$x = \begin{cases}
   	\frac{(\frac{1-p}{p}) - 1}{(\frac{1-p}{p})^c - 1}, \quad &p \neq \frac{1}{2}\\
   	\frac{1}{c}, \quad &p=\frac{1}{2}
    \end{cases}$$
	We then obtain our final \tb{Gambler's Ruin formula}:
	$$\blue{s(a) = \begin{cases}
    		\frac{(\frac{1-p}{p})^a - 1}{(\frac{1-p}{p})^c - 1} , \quad &p \neq \frac{1}{2} \\
    		\frac{a}{c}, \quad & p = \frac{1}{2}
    		\end{cases}}$$
    \remark
    We will sometimes write $s(a)$ as $s_{c,p}(a)$, to show the explicit dependence on $c$ and $p$.
    \example
    $c = 10,000, a = 9,700, p = 0.5$, then
    $$s(a) = a/c = 0.97$$
    \example
    $c = 10,000, a = 9,700, p = 0.49$, then
    $$s(a) \approx \frac{1}{163,000}$$
    
    \proposition[\label{gb}] Let $T = \min(T_0, T_c)$ be the time when the Gambler's Ruin game ends. Then $P(T > mc) \leq (1-p^c)^m$ where $m \in \mb{Z}^+$ and $P(T = \infty) = 0$, and $\expect{T} < \infty$.
    \begin{proof}
    	(1) If the player ever wins $c$ bets in a row, then the game must be over. \\
    	Then if $T > mc$, then the player has failed to win $c$ bets in a row, despite having $m$ independent attempts to do so.\\
    	But the probability of winning $c$ bets in a row is $p^c$. So the probability of failing to win $c$ bets in a row is $1 - p^c$. Therefore the probability of failing on $m$ independent attempts is $(1-p^c)^m$, as claimed. \\
    	(2) Then by continuity of probabilities,
    	$$P(T = \infty) = \limit{m} P(T>mc) \leq \limit{m}(1-p^c)^m = 0$$
    	(3) We have
    	\begin{align*}
    		E(T) &= \sum_{i=1}^\infty P(T \geq i)\\
    		&\leq \sum_{i=0}^\infty P(T \geq i)\\
    		&= P(T \geq 0) + P(T \geq 1) + P(T \geq 2) + P(T \geq 3) + P(T \geq 4) + \hdots \\
    		&\leq P(T \geq 0) + P(T \geq 0) + \hdots + P(T \geq 0) + P(T \geq c) + P(T \geq c) + \hdots\\
    		&= \sum_{j=0}^\infty c P(T \geq cj) \\
    		&\leq \sum_{j=0}^\infty c(1-p^c)^j \\
    		&= \frac{c}{1-(1-p^c)} \\
    		&= \frac{c}{p^c} < \infty
    	\end{align*}
    \end{proof}
    
   \remark This says that, with probability 1 the Gambler's Ruin game must eventually end, and the time it takes to end has finite expected value.
    
    \subsection{Mean Recurrence Times}
    \definition[mean recurrence time] The \under{mean recurrence time} of a state $i$ is
    $$m_i = E_i(\inf\{n \geq 1: X_n = i\}) = E_i(\tau_i)$$
    where $\tau_i = \inf\{n \geq 1: X_n = i\}$
    \remark
    That is, $m_i$ is the expected value of the time to return from $i$ back to $i$.
    \definition[positive recurrence and null recurrence]
    A state is \under{positive recurrent} if $m_i < \infty$. It is \under{null recurrent} if it is \blue{recurrent} but $m_i = \infty$.
    \theorem[Recurrence Time Theorem] For an irreducible Markov chain, either
    \begin{enumerate}
    	\item $m_i < \infty$ for all $i \in S$, and there is a \red{unique} stationary distribution given by $\pi_i = 1/m_i$; or
    	\item $m_i = \infty$ for all $i \in S$, and there is \red{no} stationary distribution.
    \end{enumerate}
    
    \proposition An irreducible Markov chain on a \blue{finite} state space $S$ always falls into case $(i)$ above:\\
    $m_i < \infty$ for all $i \in S$, and there is a \red{unique} stationary distribution given by $\pi_i = 1/m_i$.
    \remark
    The converse is false: There could be an example that has infinite state space $S = \mb{N}$, but still has a stationary distribution, so it falls into case $(i)$.
    \subsection{Application - Sequence Waiting Times}
    \paragraph{Problem}
    Suppose we repeatedly flip a fair coin and get Heads(H) or Tails(T) independently each time with probability $1/2$ each. Let $\tau$ be the first time the sequence $HTH$ is completed. What is $E[\tau]$?\\
    To find $E[\tau]$, we can use Markov chains.\\
    Let $X_n$ be the partial amount of the desired sequence ($HTH$) that the chain has ``achieved so far" after $n$ flips. Then we always have $X_\tau = 3$, since we ``win" upon reaching state 3. Assume we ``start over" right after we win ($X_{\tau + 1} = 1$ if flip ($\tau + 1$) is Heads, otherwise $X_{\tau+1} = 0$). Also, we take $X_0 = 0$, i.e., at the beginning we have not achieved any of the sequence.\\
    Here, $\{X_n\}$ is a Markov chain with state space $S= \{0, 1, 2, 3\}$ and $P = \begin{pmatrix}
    	1/2 & 1/2 & 0 & 0\\
    	0 & 1/2 & 1/2 & 0\\
    	1/2 & 0 & 0 & 1/2\\
    	1/2 & 1/2 & 0 & 0
    \end{pmatrix}$. \blue{The mean waiting time of $HTH$ is thus equal to the mean recurrence time of state $3$.}\\
    Using the equation $\pi P = \pi$, it can be computed that the stationary distribution is $(0.3, 0.4, 0.2, 0.1)$. Therefore, by the Recurrence Time Theorem, the mean time to return from state 3 to state 3 (has the same probability as going from state 0 to state 3) is $1/\pi_3 = 10$.
    
    
    \section{Martingales}
    Roughly speaking, martingales are stochastic processes which ``stays the same on average".
    \subsection{Martingale Definitions}
    For a formal definition, let $\{X_n\}_{n=0}^\infty$ be a sequence of random variables. We assume throughout that random variables $X_n$ have \tb{finite expectation} (or are \tb{integrable}): $E|X_n| < \infty \quad \forall n$.
    \definition[Martingale]
    A sequence $\{X_n\}_{n=0}^\infty$ is a \under{martingale} if for all $n$,
    $$E(X_{n+1}|X_0, \hdots, X_n) = X_n$$
    \remark
    No matter what has happened so far, the average of the next value will be equal to the most recent one. \\
    \paragraph{Special case: Markov chain}
    If the sequence $\{X_n\}$ is a Markov chain, then we have
    \begin{align*}
    	E[X_{n+1} | X_0 = i_0, \hdots, X_n = i_n] &= \sum_{j \in S}jP[X_{n+1} | X_0 = i_0, \hdots, X_n = i_n]\\
    	&= \sum_{j}jP[X_{n+1} |X_n = i_n] \\
    	&= \sum_{j}jp_{i_n,j}
    \end{align*}
    To be a martingale, this value must equal $i_n$. That is, a Markov chain (with $E|X_n| < \infty$) is a martingale if
    $$\sum_{j \in S}jp_{ij} = i$$ for all $i \in S$.
    \example[simple symmetric random walk]
    Let $\{X_n\}$ be s.s.r.w. with $p = 1/2$. We always have $|X_n| \leq n$, so $E|X_n| \leq n < \infty$, so there is no problem with finite expectations.\\
    For all $i \in S$, we compute that $\sum_{j \in S}jp_{ij} = (i+1)(1/2) + (i-1)(1/2) = i$, so s.s.r.w. is indeed a martingale.
    \proposition If $\{X_n\}$ is a martingale, then by the Law of Total Expectation,
    $$E(X_{n+1}) = E[E(X_{n+1}|X_0, X_1, \hdots, X_n)] = E(X_n)$$
    $$\implies E(X_n) = E(X_0) \quad \forall n$$
    This is not surprising, since martingales stay the same on average. However, this is not a sufficient condition for $\{X_n\}$ to be a martingale.
    \subsection{Stopping Times}
    We often want to consider $E(X_T)$ for a random time $T$. We need to prevent the random time $T$ from looking into the future of the process, before deciding whether to stop.
    \definition[stopping time] A non-negative-integer-valued random variable $T$ is a \under{stopping time} for $\{X_n\}$ if the event $\{T = n\}$ is determined by $X_0, X_1, \hdots, X_n$, i.e. if the indicator function $\id{T=n}$ is a function of $X_0, X_1, \hdots, X_n$.
    \remark
    Intuitively, this definition says that a stopping time $T$ must decide whether to stop at time $n$ based solely on what has happened up to time $n$, without first looking into the future.
    \example
    valid stopping times:\\
    $T=5, T=\inf\{n \geq 0: X_n = 5\}, T = \inf \{n \geq 0: X_n = 0 \lor X_n = c\}, T = \inf \{n \geq 2: X_{n-2} = 5\}$
    not valid stopping time:
    $T = \inf\{n \geq 0: X_{n+1} = 5\}$ (since it looks into the future)
    \lemma[Optional Stopping Lemma] If $\{X_n\}$ is a martingale, and $T$ is a stopping time which is \blue{bounded} (i.e., $\exists M < \infty$ with $P(T \leq M) = 1$), then 
    $$E(X_T) = E(X_0)$$
    \todo{prove this!}
    \example
    Consider s.s.r.w. with $X_0 = 0$, and let
    $$T = \min\{ 10^{12}, \inf\{ n \geq 0: X_n = -5\}$$
    Then $T$ is a bounded stopping time. Hence by the Optional Stopping Lemma, 
    $$E(X_T) = E(X_0) = E(0) = 0$$
    But near always, we will have $X_T = -5$.\\
    By the Law of Total Expectation,
    \begin{align*}
    	0 &= E(X_T) \\
    	&= \underbrace{P(X_T = -5)}_{\approx 1}\underbrace{E(X_T|X_T = -5)}_{=-5} + \underbrace{P(X_T \neq -5)}_{\approx 0}\underbrace{E(X_T|X_T \neq -5)}_{huge}
    \end{align*}
    \theorem[\label{Optional Stopping Theorem}Optional Stopping Theorem] If $\{X_n\}$ is a martingale with stopping time $T$, and $P(T < \infty) = 1$, and $E|X_T| < \infty$, and $\limit{n} E(X_n \id{T > n}) = 0$, then
    $$E(X_T) = E(X_0)$$
    \begin{proof}
    	For each $m \in \mb{N}$, let $S_m = \min\{T, m\}$, so that $S_m$ is a bounded stopping time. \\
    	Then by Optional Stopping Lemma, $E(X_{S_m}) = E(X_0)$ (for any $m$). \\
    	Then for any $m$,
    	\begin{align*}
    		X_{S_m} &= X_{\min(T,m)} \\
    		&= X_T\id{T\leq m} + X_m\id{T>m} \\
    		&= X_T(1 - \id{T>m}) + X_m\id{T>m} \\
    		&= X_T - X_T\id{T>m} + X_m\id{T>m} \\
    		\implies X_T &= X_{S_m} + X_T\id{T>m} - X_m\id{T>m} \\
    		\implies E(X_T) &= E(X_{S_m}) + E(X_T\id{T>m}) - E(X_m\id{T>m}) \\
    		&= E(X_0) + E(X_T\id{T>m}) - E(X_m\id{T>m})
    	\end{align*}
    	Take $m\rightarrow \infty$. Since $P(T<\infty) = 1$, we have $\id{T>m} \rightarrow 0$.\\
    	Since $E|X_T| < \infty$ and $\id{T>m} \rightarrow 0$, we have
    	$$\limit{m} E(X_T\id{T>m}) = 0$$
    	by the Dominated Convergence Theorem \ref{Dominated Convergence Theorem} \\
    	Also, $\limit{m}E(X_m\id{T>m}) = 0$ by assumption.\\ 
    	Hence $E(X_T) \rightarrow E(X_0)$, i.e. $E(X_T) = E(X_0)$.
    \end{proof}
    \corollary[\label{Optional Stopping Corollary}Optional Stopping Corollary] If $\{X_n\}$ is a martingale with stopping time $T$, which is ``bounded up to time $T$" (i.e., $\exists M < \infty$ with $P(|X_n|\id{n\leq T} \leq M) = 1$ for all $n$), and $P(T<\infty) = 1$, then 
    $$E(X_T) = E(X_0)$$
    \begin{proof}
    	It follows that, $P(|X_T| \leq M) = 1$.\\
    	Hence, $E|X_T| \leq M < \infty$.\\
    	Also, 
    	\begin{align*}
    		|E(X_n\id{T>n})| &\leq E(|X_n|\id{T>n})\\
    		&= E(|X_n|\id{n\leq T}\id{T>n})\\
    		&\leq E(M\id{T>n}) \\
    		&= MP(T>n) \rightarrow 0 \tag{Since $P(T<\infty) = 1$}
    	\end{align*}
    	Hence the result follows from the Optional Stopping Theorem.
    \end{proof}
    
    \example[Gambler's Ruin problem - $p = 1/2$]
    Let $T = \inf\{ n \geq 0:X_n \lor X_n = c\}$ be the time when the game ends. Then $P(T<\infty) = 1$ by Proposition \ref{gb}. Also, if the game has not yet ended, i.e. $n \leq T$, then $X_n$ must be between $0$ and $c$. Hence $|X_n|\id{n \leq T}\leq c < \infty$ for all $n \leq T$.\\
    So by the Optional Stopping Corollary \ref{Optional Stopping Corollary}, $E(X_T) = cs(a) + 0(1-s(a) = E(X_0) = a \implies s(a) = a/c$.
    \example[Gambler's Ruin problem - $p \neq 1/2$]
    Then $\{X_n\}$ is not a martingale since 
    $$\sum_{j}jp_{ij} = p(i+1) + (1-p)(i-1) = i + 2p - 1 \neq i$$
    Instead we use a trick: Let $Y_n := \left( \frac{1-p}{p}  \right)^{X_n}$, then $\{Y_n\}$ is also a Markov chain, and
    \begin{align*}
    	E(Y_{n+1}|Y_0, Y_1, \hdots, Y_n) &= p\left( \frac{1-p}{p}\right)^{X_n + 1} + (1-p)\left( \frac{1-p}{p}   \right)^{X_n - 1} \\
    	&= p\left[ Y_n \left( \frac{1-p}{p}\right) \right] + (1-p)\left[ Y_n / \left( \frac{1-p}{p}\right) \right] \\
    	&= Y_n (1-p) + Y_n(p) \\
    	&= Y_n
    \end{align*}
    So $\{Y_n\}$ is a martingale.\\
    Again, $P(T<\infty) = 1$ by Proposition \ref{gb}. \\
    Also, $|Y_n|\id{n\leq T} \leq \max\left(  \left( \frac{1-p}{p}\right)^0, \left( \frac{1-p}{p}\right)^c \right) := M < \infty$ for all $n$. Hence by the Optional Stopping Corollary \ref{Optional Stopping Corollary}, $$E(Y_T) = s(a)\left( \frac{1-p}{p}\right)^c + [1-s(a)](1) = E(Y_0) = \left( \frac{1-p}{p}\right)^a$$
    $$\implies s(a) = \frac{\left( \frac{1-p}{p}\right)^a-1}{\left( \frac{1-p}{p}\right)^c - 1}$$
    \subsection{Wald's Theorem}
    Suppose $X_n = a + Z_1 + \hdots + Z_n$, where $\{Z_i\}$ are i.i.d. with finite mean $m$. Let $T$ be a stopping time for $\{X_n\}$ which has finite mean, i.e. $E(T) < \infty$. Then
    $$E(X_T) = a + mE(T)$$
    \property[Special case: $m = 0$]
    Then $\{X_n\}$ is a martingale, and Optional Stopping Theorem \ref{Optional Stopping Theorem} says that $E(X_T) = a = E(X_0)$. \\
    \todo{prove this!}
    \corollary If $\{X_n\}$ is Gambler's Ruin with $p \neq 1/2$, and $T = \inf\{n \geq 0: X_n = 0 \lor X_n = c\}$, then
    $$E(T) = \frac{1}{2p-1}\left( c \frac{\left( \frac{1-p}{p}\right)^a - 1}{\left( \frac{1-p}{p}\right)^c - 1}-a\right)$$
    
    \begin{proof}
    	We again apply Wald's Theorem:\\
    	Here $Z_i = +1$ if you win the $i$th bet, otherwise $Z_i = -1$. So $$m = E(Z_i) = p(1) + (1-p)(-1) = 2p-1$$ Also, $E(T) < \infty$ by Proposition \ref{gb}. Then by Wald's Theorem,
    	\begin{align*}
    		E(X_T) &= a + mE(T) \\
    		&= cs(a) + 0(1-s(a)) \\
    		&= c \frac{\left( \frac{1-p}{p}\right)^a - 1}{\left( \frac{1-p}{p}\right)^c - 1}\\
    		\implies E(T) &= \frac{1}{m}(E(X_T) - a) \\
    		&= \frac{1}{2p-1}\left( c \frac{\left( \frac{1-p}{p}\right)^a - 1}{\left( \frac{1-p}{p}\right)^c - 1}-a\right)
    	\end{align*}
    \end{proof}
    
    \lemma Let $X_n = a + Z_1 + \hdots + Z_n$, where $\{Z_i\}$ are i.i.d. with mean 0 and variance $v < \infty$. Let $Y_n = (X_n - a)^2 - nv = (Z_1 + \hdots + Z_n)^2 - nv$. Then $\{Y_n\}$ is a martingale.
    \todo{prove this!}
    \corollary If $\{X_n\}$ is Gambler's Ruin with $p = 1/2$, and $T = \inf\{n \geq 0: X_n = 0 \lor X_n = c\}$, then 
    $$E(T) = Var(X_T) = a(c-a)$$
    \todo{prove this!}
    
    \subsection{Application - Sequence Waiting Times}
    Suppose at each time $n$, a new ``player" appears, and bets $\$ 1$ on heads, then if they win they bet $\$2$ on tails, then if they win again they bet $\$4$ on heads. Each player stops betting as soon as they either lose once (and hence are down a total of $\$1$), or win three bets in a row (and hence are up a total of $\$7$.\\
    Let $X_n$ be the total amount won by all the betters by time $n$. Then since the bets were fair, $\{X_n\}$ is a martingale with stopping time $\tau$. 
    
    \subsection{Martingale Convergence Theorem}
    Suppose $\{X_n\}$ is a martingale. Then $\{X_n\}$ could have infinite fluctuations in both directions, as we have seen for s.s.r.w.; Or $\{X_n\}$ could converge with probability 1 to a fixed (perhaps random) value.
    \example
    Let $\{X_n\}$ be Gambler's Ruin with $p = 1/2$, where we \blue{stop} as soon as we either win or lose. Then $X_n \rightarrow X$ with probability 1, where $P(X = c) = a/c$ and $P(X=0) = 1 - a/c$.
    \example
    Let $\{X_n\}$ be a Markov chain on $S = \{2^m: m \in \mb{Z}\}$, with $X_0 = 1$, and $p_{i,2i} = 1/2$ and $p_{i, i/2} = 2/3$ for $i \in S$. This is a martingale, since $\sum_{j} jp_{ij} = (2i)(1/3) + (i/2)(2/3) = i$.\\
    Let $Y_n = \log_2 X_n$. Then $Y_0 = 0$, and $\{Y_n\}$ is s.r.w. with $p = 1/3$, $Y_n \rightarrow -\infty$ w.p. 1 by the Law of Large Numbers \ref{Law of Large Numbers}. Hence, $X_n = 2^{Y_n} \rightarrow 2^{-\infty} = 0$ w.p. 1.
    \theorem[\label{Martingale Convergence Theorem}Martingale Convergence Theorem] Any non-negative martingale $\{X_n\}$ ($X_n \geq 0$) which is bounded below (i.e. $X_n \geq c$ for all $n$, for some finite number $c$), or is bounded above (i.e. $X_n \leq c$ for all $n$, for some finite number $c$), converges w.p. 1 to some random variable $X$.
    \remark
    The intuition behind this theorem is:
    \begin{enumerate}
    	\item Since the martingale is bounded on one side, it cannot ``spread out" forever.
    	\item Since it is a martingale, it cannot ``drift" in a positive or negative direction.
    	\item So it has somewhere to go, and eventually has to stop somewhere.
    \end{enumerate}
   	\remark
   	If $\{X_n\}$ is not non-negative, then if $X_n \geq c$, then $\{X_n - c\}$ is a non-negative martingale, or if $X_n \leq c$, then $\{-X_n + c\}$ is a non-negative martingale, and in either case the non-negative martingale converges iff $\{X_n\}$ converges.
   	\subsection{Application - Branching Processes}
   	\definition[offspring distribution]
   	Let $\mu$ be any prob dist on $\{0, 1, 2, \hdots\}$, the \under{offspring distribution}. Let $X_n$ be the number of individuals at time $n$. Start with $X_0 = a$ individuals. Assume $0 < a < \infty$, $Z_{n,i} \overset{i.i.d.}{\sim} \mu(i)$.\\ (i.e., Each of the $X_n$ individuals at time $n$ has a random number of offspring under the distribution $\mu$). Then 
   	$$X_{n+1} = Z_{n,1} + Z_{n,2} + \hdots + Z_{n,X_n}$$
   	Here $\{X_n\}$ is a Markov chain, on the state space $\{0, 1, 2, \hdots\}$.\\
   	\paragraph{Transition probabilties}
   	If $X_n$ ever reaches 0, then it stays there forever: $p_{0j} = 0\quad \forall j\geq0$. This is called \under{extinction}.\\ Also, $p_{ij} = (\mu \ast \mu \ast \hdots \ast \mu)(j)$, a \under{convolution} of $i$ copies of $\mu$.\\
   	\theorem
   	Let $m = \sum_{i}\mu(i)$ be the mean of $\mu$, which is called the \under{reproductive number}. If $m < 1$, then $E(X_n) \rightarrow 0$, and $P(X_n = 0) \rightarrow 1$.
   	\begin{proof}
   		   	Assume $0 < m < \infty$. Then
   	$$E(X_{n+1}|X_0, \hdots, X_n) = E(Z_{n,1} + Z_{n,2} + \hdots + Z_{n,X_n}|X_0, \hdots, X_n) = mX_n$$
   	$$\implies E(X_n) = m^nE(X_0) = m^n a < \infty$$
   	So if $m < 1$, then $E(X_n) = am^n \rightarrow 0$. Then we have
   	\begin{align*}
   		E(X_n) &= \sum_{k=0}^\infty kP(X_n = k) \\
   		&\geq \sum_{k=1}^\infty P(X_n = k) \\
   		&= P(X_n \geq 1) \\
   		\implies P(X_n \geq 1) &\leq E(X_n) = am^n \rightarrow 0 \\
   		\implies P(X_n = 0) &\rightarrow 1
   	\end{align*}
   	\end{proof}
   	\fact   Let $m = \sum_{i}\mu(i)$ be the mean of $\mu$, which is called the \under{reproductive number}. If $m > 1$, then $E(X_n) \rightarrow \infty$, $P(X_n \rightarrow \infty) > 0$ and $P(X_n \rightarrow 0) > 0$, i.e., we have possible extinction but also possible flourishing.
   	\theorem  Let $m = \sum_{i}\mu(i)$ be the mean of $\mu$, which is called the \under{reproductive number}. If $m = 1$, and $\mu$ is non-degenerate (i.e. $\mu(1) < 1$, so that $\mu$ is not a constant), then $\{X_n\} \rightarrow 0$ w.p. $1$.
   	\begin{proof}
   		If $m=1$, then $E(X_n) = E(X_0) = a$ for all $n$. Then $E(X_{n+1}|X_0, \hdots, X_n) = mX_n - X_n$, so $\{X_n\}$ is a non-negative martingale.\\
   		Hence by the Martingale Convergence Theorem \ref{Martingale Convergence Theorem}, we must have $X_n \rightarrow X$ for some random variable $X$. This could only happen if
   		\begin{enumerate}
   			\item $\mu(1) = 1$; or
   			\item $X = 0$
   		\end{enumerate}
   	\end{proof}
   	
   	
   	\subsection{Application - Stock Options (Discrete)}
   	In mathematical finance, it is common to model the price of one share of some stock as a random process.\\
   	For now, we work in discrete time, and suppose that $X_n$ is the price of one share of the stock at each date $n$. If you buy the stock, then the situation is clear: if $X_n$ increases then you will make a profit, but if $X_n$ decreases then you will suffer a loss.
   	\definition[stock option]
   	A \under{stock option} is the option to buy one share of the stock for some fixed strike price $K$ at some fixed future strike date (time) $S > 0$.
    
    
    
    
    
    
    
    
    
\end{document}