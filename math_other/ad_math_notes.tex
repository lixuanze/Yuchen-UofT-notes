\documentclass[11pt]{article}
% Libraries.

\usepackage{dsfont}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{esint}
\usepackage[margin=3cm]{geometry}
%\usepackage{pgfplots}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{perpage}
\usepackage[dvipsnames, pdftex]{xcolor}
\usepackage{float}
\usepackage{xargs}
\usepackage{../raina}
\usepackage[
	colorinlistoftodos,
	prependcaption,
	textsize=tiny
]{todonotes}
\usepackage{subcaption}



% Property settings.
\MakePerPage{footnote}
\pagestyle{headings}
\counterwithin{equation}{section}


% Attr.
\title{Advanced Math Notes}
\author{Yuchen Wang}
\date{\today}

\begin{document}
	\maketitle
	\tableofcontents
	\newpage
	\section{Free parameter}
	A variable in a mathematical model which cannot be predicted precisely or constrained by the model and must be estimated experimentally or theoretically.
	\section{Rectifier}
	\subsection{Definition}
	An activation function defined as the positive part of its argument:
	$$f(x) = \max(0,x)$$
	Also known as: ramp function \\
	A unit employing the rectifier is also called a \tb{rectified linear unit (ReLU)}
	\subsection{Softplus (Smooth ReLu)}
	A smooth approximation to the rectifier is the analytic function $$f(x) = \log(1+e^x)$$
	Also known as: SmoothReLU\\
	The derivative of softplus is $$f'(x) = \frac{1}{1+e^{-x}}$$(the logistic function)
	\paragraph{Notes}
	The logistic function is a smooth approximation of the derivative of the rectifier, the \tb{Heaviside step function}
	\subsection{LogSumExp: Multivariable Generalization to Softplus}
	LogSumExp with the first argument set to zero
	$$LSE_0^+(x_1,\hdots,x_n) := LSE(0,x_1,\hdots,x_n) = \log(1+e^{x_1} + \hdots + e^{x_n})$$
	\paragraph{Notes}
	The LogSumExp function itself is:
	$$LSE(x_1,\hdots,x_n) = \log(e^{x_1} + \hdots + e^{x_n})$$
	and its gradient is the softmax.\\
	The softmax with the first argument set to zero is the multivariable generalization of the logistic function.
	\section{Softmax Function}
	The softmax function takes an un-normalized vector, and normalizes it into a probability distribution. That is, prior to applying softmax, some vector elements could be negative, or greater than one; and might not sum to 1; but after applying softmax, each element $x_i$ is in the interval $[0,1]$, and $\sum_i x_i = 1$ \\
	$$\sigma:\real^K \rightarrow \{\sigma\in \real^K|\sigma_i>0,\sum_{i=1}^K \sigma_i = 1\}$$
	$$\sigma(\vz)_j = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}}$$ for $j = 1,\hdots,K$
	
	\section{Cross Entropy}
	The \under{Cross entropy} between two probability distributions $p$ and $q$ over the same underlying set of events measures the average number of bits needed to identify an even drawn from the set if a coding scheme used for the set is optimized for an estimated probability distribution $q$, rather than the true distribution $p$.
	\paragraph{Discrete distributions}
	$$ H(p, q) = - \sum_{x \in \chi} p(x) \, \log q(x) $$
	\paragraph{Continuous distributions}
	$$ H(p, q) = - \int_\chi P(x)\, \log Q(x) \, dr(x) $$
	\section{Cross Product in Higher Dimensions}
	A way of turning 3 vectors in 4-space into a fourth vector, orthogonal to the others, in a trilinear way \\
	Canonical basis of $\real^4: (e_1, e_2, e_3, e_4)$. If your vectors are $\tb{t} = (t_1, t_2, t_3, t_4), \tb{u} = (u_1, u_2, u_3, u_4)$ and $\tb{v} = (v_1, v_2, v_3, v_4)$, then compute the determinant:
	$$\begin{vmatrix}
	t_1 & t_2 & t_3 & t_4 \\
	u_1 & u_2 & u_3 & u_4 \\
	v_1 & v_2 & v_3 & v_4 \\
	e_1 & e_2 & e_3 & e_4
	\end{vmatrix}$$
	The cross product of $\tb{t}, \tb{u}, \tb{v}$ is:
		$$-e_1\begin{vmatrix}
	 t_2 & t_3 & t_4 \\
	 u_2 & u_3 & u_4 \\
	 v_2 & v_3 & v_4 \\
	\end{vmatrix}
	+e_2\begin{vmatrix}
	t_1 & t_3 & t_4 \\
	u_1 & u_3 & u_4 \\
	v_1 & v_3 & v_4 \\
	\end{vmatrix}
	- e_3\begin{vmatrix}
	t_1 & t_2 & t_4 \\
	u_1 & u_2 & u_4 \\
	v_1 & v_2 & v_4 \\
	\end{vmatrix}
	+ e_4\begin{vmatrix}
	t_1 & t_2 & t_3 \\
	u_1 & u_2 & u_3 \\
	v_1 & v_2 & v_3 \\
	\end{vmatrix}
	$$
\section{Gaussian Process}
\subsection{The Basics}
\paragraph{Definition 1}
We use a \under{Gaussian process} to describe a distribution over functions:
$$\tb{f} \sim \mathcal{GP}(m, K)$$ 
where $m: \chi \rightarrow \real$ is the \under{mean function}
$$m(\vx) = E[f(\vx)]$$
and $K: \chi^2 \rightarrow \real$ is the \under{covariance function}
$$K(\vx, \vx') = E[(f(\vx) - m(\vx))(f(\vx') - m(\vx'))]$$
\paragraph{Definition 2}
For any set $S$, a \under{Gaussian Process} on S is a set of r.v.s $Z_t: t\in S$ s.t. $\forall n \in \mathbb{N}, \forall t_1,\hdots,t_n \in S, (Z_{t_1},\hdots, Z_{t_n})$ is multi-variate Gaussian.
\paragraph{Theorem: Existence of Gaussian Process}
For any set $S$, any mean function $\mu: S \rightarrow \real$, and any covariance function $k: S\times S \rightarrow \real$, there exists a GP $Z_t$ on S s.t. $E(Z_t) = \mu(t), Cov(Z_s, Z_t) = k(s,t) \, \forall s, t\in S$.

\paragraph{GPs define multivariate Gaussian distributions}
We have data points $X = [\vx_1^T, \hdots, \vx_n^T]^T$ and are interested in their function values $\tb{f}(X) = (f(\vx_1), \hdots, f(\vx_n))^T$.\\
$\tb{f}$ is one subset of r.v. and has (prior) joint Gaussian distribution:
$$\vf(X) \sim \mathcal{N}(\vm(X),K(X,X))$$
\paragraph{Remarks}
\begin{enumerate}
	\item The covariance function $K(\vx, \vx')$ returns a measure of the similarity of $\vx$ and $\vx'$ that also encodes how similar $f(\vx)$ and $f(\vx')$ should be.
	\item The mean function $m(\vx)$ encodes a prior expectation of the (unknown) function
\end{enumerate}

\paragraph{Setting the mean function}
In most cases we simply use
$$E(f(\vx)) = m(\vx) = 0$$
which makes sense especially if we normalize the output to zero mean.
\paragraph{Properties of the covariance function}
The covariance function $K(\vx,\vx')$ needs to be a measure of similarity between $\vx$ and $\vx'$.
\begin{enumerate}
	\item K needs to be symmetric $$K(\vx, \vx') = K(\vx', \vx)$$
	\item K needs to be positive semidefinite (nonnegative definite) $$\int_{-\infty}^\infty\int_{-\infty}^{\infty} K(\vx, \vx')g(\vx)g(\vx')\,d\vx d\vx' \geq 0$$ for all $g \in L_2$.
\end{enumerate}

\paragraph{Setting the covariance function}
\begin{enumerate}
	\item \under{Gaussian Kernel}:
$$K(r) = \theta_A^2 \exp[-\frac{r^2}{2\theta_L^2}]$$
\item \under{Periodic Covariance Function}
$$K(r) = \theta_A^2\exp[-\frac{\sin^2[(2\pi/\theta_P)r]}{2}]$$
where $r = ||x-x'||$ denotes the Euclidean distance between two indexes.
\end{enumerate}
\paragraph{Hyperparameters}
$\theta_A$:  $y$-scaling \\
$\theta_L$:  $x$-scaling (or time scale if the data are time series) \\
$\theta_P$:  period of the covariance functions \\
\subsection{Gaussian Process Regression}
Basically equivalent to Bayesian linear regression.\\
Twist is that using kernel instead of basis functions in order to define the family of functions that you are using for regression.\\
This allows us to define a very rich family of functions that using basis functions alone could not handle (e.g. mapping into an infinite dimensional space).\\
In a Gaussian Process regression model, mathematically the same inference as in linear regression can be done.
\paragraph{The model}
Let $Z \in \real^n \sim N(\mu, K), \varepsilon \in \real^n \sim N(0, \sigma^2I)$ be independent r.v.s. Let $y = Z + \varepsilon$, so $y \sim N(\mu, K+\sigma^2I)$.\\
Define $C = K + \sigma^2 I$.\\
Let $a = (1, \hdots, l), b = (l+1, \hdots, n)$, so $y = \begin{pmatrix}
	y_a \\ y_b \end{pmatrix}$, where $y_a = \begin{pmatrix} y_1 \\ \vdots \\y_l
	\end{pmatrix}, y_b = \begin{pmatrix} y_{l+1} \\ \vdots \\ y_n \end{pmatrix}$.
	In addition, $\mu = \begin{pmatrix} \mu_a \\ \mu_b \end{pmatrix}, C = \begin{pmatrix} C_{aa} & C_{ab} \\ C_{ba} & C_{bb} \end{pmatrix}, K = \begin{pmatrix} K_{aa} & K_{ab} \\ K_{ba} & K_{bb} \end{pmatrix}$\\
Then we have $(Y_a | Y_b = y_b) \sim N(m, D)$, where 
\begin{align*}
	m &= \mu_a + C_{ab}C^{-1}_{bb}(y_b - \mu_b) \\
	&= \mu_a +K_{ab}(K_{bb} + \sigma^2 I)^{-1}(y_b - \mu_b)\\
	D &= C_{aa} - C_{ab}C^{-1}_{bb}C_{ba} \\
	&= (K_{aa}+\sigma^2 I)-K_{ab}(K_{bb}+\sigma^2I)^{-1}K_{ba}
\end{align*}
\paragraph{Parameters}
\begin{enumerate}
	\item $\mu$
	\item $K$
\end{enumerate}
\paragraph{Inference}
\begin{enumerate}
	\item Plot the mean function to predict unobserved values (good for visualization)
	\item Plot the error curves
	\item Choose a loss function and minimize the loss using posterior distribution
\end{enumerate}
\paragraph{Negative Log Marginal Likelihood (NLML)}
The values of hyperparameters $\theta$ may be optimized by minimizing NLML:
\begin{align*}
	NLML &= -\log p(\vy|\vx,\theta) \\
	&= \frac{1}{2}\log |K| + \frac{1}{2}\vy^TK^{-1}\vy + \frac{n}{2} \log (2\pi)
\end{align*}
\section{Kronecker Product}
A generalization of the outer product from vectors to matrices.
\paragraph{Definition}
$$A \otimes B = \begin{bmatrix}
	a_{11}B & \hdots & a_{1n}B \\
	\vdots & \ddots & \vdots \\
	a_{m1}B & \hdots & a_{mn}B
\end{bmatrix}$$
\section{Bayes' Theorem}
$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$
where $A$ and $B$ are events and $P(B) \neq 0$.
\section{Maximum a Posteriori Estimation (MAP)}
An estimate of an unknown quantity that equals the mode of the posterior distribution. \\
Can be used to obtain a \textcolor{blue}{point estimate} of an unobserved quantity on the basis of empirical data.\\
Closely related to MLE but employs an augmented optimization objective which incorporates a prior distribution (fix the overfitting problem). \\
Can be seen as a regularization of MLE.
\paragraph{Description}
Assume we want to estimate an unobserved population parameter $\theta$ on the basis of observations $x$> Let $f$ be the sampling distribution of $x$ so that $f(x|\theta)$ is the probability of $x$ when the underlying population parameter is $\theta$. \\
Now assume that a prior distribution $g$ exists. This allows us to treat $\theta$ as a random variable. By Bayes' Theorem, the posterior distribution of $\theta$ is 
$$f(\theta|x) = \frac{f(x|\theta)g(\theta)}{\int_\Theta f(x|v)g(v)\,dv}$$
where $g$ is the density function of $\theta$, $\Theta$ is the domain of $g$.
Then
$$\hat{\theta}_{MAP}(x) = \underset{\theta}{argmax} f(\theta|x)$$
\section{Bayesian Linear Regression}
Why not use MLE? - overfitting \\
Why not use MAP? - no representation of uncertainty
\paragraph{Setup}
$$D = ((x_1, y_1), \hdots, (x_n, y_n)), x_i \in \real^d, y_i \in \real$$
\paragraph{Model}
$y_1, \hdots, y_n$ indep given $w$, $y_i \sim N(w^Tx_i, a^{-1}), a>0$ \\
$w \sim N(0,b^{-1}I), b>0. \, (w=(w_1,\hdots,w_d))$ \\
($a, b:= 1/ variance$ is called precision) \\
Assume $a,b$ are known so the only parameter is $w$.
\paragraph{Posterior Distribution of $w$}
It can be shown that $$P(w|D) = N(w|\mu, \Lambda^{-1})$$
where $\mu = a\Lambda^{-1}X^Ty$ and $\Lambda = aX^TX+bI$ where $X$ is the design matrix.
\section{Laplace Distribution}
Sometimes also called ``double exponential distribution", because it can be thought of as two exponential distributions (with an additional location parameter) spliced together back-to-back.
\paragraph{PDF}
$$\frac{1}{2b}\exp(-\frac{|x-\mu|}{b})$$
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.3]{./images/laplace.png}
	\caption{PDF of Laplace Distribution}
\end{figure}

\section{Kullback-Leibler Divergence}
Also called ``relative entropy", is a measure of how one probability distribution is different from a second, reference probability distribution.
\paragraph{Definition}
For discrete probability distribution $P$ and $Q$ defined on the same probability space, the \under{Kullback-Leibler divergence} between $P$ and $Q$ is defined to be
$$D_{KL}(P|Q) = - \sum_{x \in \chi} P(x)\log(\frac{Q(x)}{P(x)})$$
For distributions $P$ and $Q$ of a continuous random variable, the Kullback-Leiber divergence is defined to be the integral
$$D_{KL}(P|Q) = \int_{-\infty}^\infty p(x)\log(\frac{p(x)}{q(x)})\, dx$$ where $p$ and $q$ denote the probability densities of $P$ and $Q$.

\section{Power Set}
Given a set S, the power set of S, is the set of all subsets of S. \\
Notation: $2^S$ or $\mathcal{P}(S)$

\section{the Binomial Theorem}
$$(x+y)^n = \sum_{k=0}^n \binom{n}{k} x^{n-k}y^k$$

\section{Markov Process}
\subsection{Introduction}
\paragraph{Definition 15.1.1 Stochastic Process}
A \under{stochastic process} is a sequence of events in which the outcome at any stage depends on some probability.

\paragraph{Definition 15.1.2 Markov Process}
A \under{Markov process} is a stochastic process with the following properties:
\begin{enumerate}
	\item The number of possible outcomes or states is finite
	\item The outcome at any stage depends only on the outcome of the previous stage
	\item The probabilities are constant over time
\end{enumerate}
 
\paragraph{Definition 15.1.3 Markov Chain}
If $\vx_0$ is a vector which represents the initial state of a system, then there is a matrix $M$ such that the state of the system after one iteration is given by the vector $M\vx_0$. Thus we get a chain of state vectors:
$$\vx_0, M\vx_0, M^2\vx_0, \hdots$$
where the state of the system after $n$ iterations is given by $M^n\vx_0$. Such a chain is called a \under{Markov chain} and the matrix M is called a \under{transition matrix}. \\
The state vectors can be of one of two types: an absolute vector or a probability vector.\\ 
An \under{absolute vector} is a vector whose entries give the actual number of objects in a given state. \\
A \under{probability vector} is a vector where the entries give the percentage (or probability) of objects in a given state. \textcolor{blue}{Note that the entries of a probability vector add up to 1.}
\paragraph{Theorem 15.1.4}
Let $M$ be the transition matrix of a Markov process such that $M^k$ has only positive entries for some $k$. Then there exists a unique probability vector $\vx_s$ such that $$M\vx_s = \vx_s$$
Moreover
$$\underset{k \rightarrow \infty}{\lim} M^k\vx_0 = \vx_s$$
for any initial state probability vector $\vx_0$.

\subsection{The Transition Matrix and its Steady-state Vector}
The transition matrix of an $n$-state Markov process is an $n\times n$ matrix $M$ where the $i,j$ entry of $M$ represents the probability that an object in state $j$ transitions into state $i$, that is if $M = (\vm_{ij})$ and the states are $S_1, S_2, \hdots, S_n$ then $\vm_{ij}$ is the probability that an object in state $S_j$ transitions to state $S_i$.

\section{Jensen's Inequality (Probability Theory)}
If X is a random variable and $\varphi$ is a convex function, then
$$\varphi(E[X]) \leq E[\varphi(X)]$$

\section{Weight Decay}
An example of a regularization method. \\
One way to penalize complexity would be to add all weights to the loss function. \\
If we have the cost function $E(\vw)$, then change the cost function to
$$\tilde{E}(\vw) = E(\vw) + \frac{\lambda}{2} \vw^2$$
Where $\lambda$ is the regularization parameter.\\
Applying gradient descent to this new cost function we obtain:
$$w_i \leftarrow w_i - \eta \frac{\partial E}{\partial w_i} - \eta\lambda w_i$$
\section{De Moivre's Formula}
For any real number $x$ and integer $n$, it holds that
$$(\cos (x) + i\sin(x))^n = \cos (nx) + i\sin(nx)$$
where $i$ is the imaginary unit ($i^2 = -1$).
\paragraph{Relation to Euler's formula}
De Moivre's formula is a precursor to Euler's formula
$$e^{ix} = \cos x + i \sin x$$
One can derive de Moivre's formula using Euler's formula and the exponential law for integer powers $${\left(e^{i x}\right)^{n}=e^{i n x}} \\ $$
since Euler's formula implies that the left side is equal to $(\cos x+i \sin x)^{n}$ while the right side is equal to  $e^{i n x}=\cos (n x)+i \sin (n x)$.


\section{Bayesian Optimization}
We are interested in finding the minimum of a function $f(x)$ on some bounded set $\chi$> which we will take to be a subset of $\real^D$. Bayesian optimization constructs a probabilistic model for $f(x)$ and then exploits this model to make decisions about where in $\chi$ to next evaluate the function, while integrating out uncertainty. \\
There are two major choices made when performing Bayesian optimization
\begin{enumerate}
	\item Select a prior (assumptions) over functions being optimized;
	\item Choose an \tb{acquisition function}, which is used to construct a utility function from the model posterior, allowing us to determine the next point to evaluate.
\end{enumerate}

\subsection{Gaussian Processes (GP) as the prior}
We will take GP to be of the form $f: \chi \rightarrow \real$. The GP is defined by the property that any finite set of $N$ points $\{\vx \in \chi\}_{n=1}^N$ induces a multivariate Gaussian distribution on $\real^N$. The $n$th of these points is taken to be the function value $f(\vx_n)$.\\
We assume that the function $f(\vx)$ is drawn from a Gaussian process prior and that our observation are of the form $\{\vx_n, y_n\}_{n=1}^N$, where $y_n \sim \mc{N}(f(\vx_n), v)$ and $v$ is the variance of noice introduced into the function observations. This prior and data induce a posterior over functions. \\
The acquisition function, which we denote by $a: \chi \rightarrow \real^+$, determines what point in $\chi$ should be evaluated next via a proxy optimization $\vx{next} = \argmax_\vx a(\vx)$, where several different functions have been proposed. In general, these acquisition functions depend on the previous observations and GP hyperparameters. We denote this dependence as $a(\vx; \{\vx_n, y_n\}, \theta)$.\\
Define $\gamma$ to be the function that normalizes $f(\vx_{best})$:
$$\gamma(\vx) = \frac{f(\vx_{best}) - \mu(\vx; \{\vx_n, y_n\}, \theta)}{\sigma(\vx; \{\vx_n, y_n\}, \theta)}$$
\paragraph{Probability of Improvement (PI)} One strategy is to maximize the probability of improving over the best current value. \\
$$a_{PI}(\vx) = P(f(\vx) <  \gamma(\vx))$$
Under GP,
$$a_{PI}(\vx; \{\vx_n, y_n\}, \theta) = \Phi(\gamma(x))$$
\paragraph{Expected Improvement} Alternatively, one could choose to maximize the expected improvement over the best current value. \\
$$a_{EI}(\vx) = \expect{\max(\gamma(\vx) - f(\vx), 0))}$$
(The idea: if the new value is much better, we win by a lot; if it's much worse, we haven't lost anything)\\
Under GP,
$$a_{EI}(\vx; \{\vx_n, y_n\}, \theta) = \sigma(\vx; \{\vx_n, y_n\}, \theta)(\gamma(\vx) \Phi(\gamma(\vx)) + \mc(\gamma(\vx); 0, 1))$$

\subsection{Jointly Continuity and Separately Continuity}
\definition[jointly continuity and separately continuity] 
If $X, Y$ and $Z$ are topological spaces and $f: X \times Y \rightarrow Z$ is a function then we say that $f$ is \tb{jointly continuous} at $\left(x_{0}, y_{0}\right) \in X \times Y$ if for each neighbourhood $W$ of $f\left(x_{0}, y_{0}\right)$ there exists a product of open sets $U \times V \subseteq X \times Y$ containing $\left(x_{0}, y_{0}\right)$ such that $f(U \times V) \subseteq W$. \\\\
We say that $f$ is \tb{separately continuous} on $X \times Y$ if for each $x_{0} \in X$ and $y_{0} \in Y$ the functions $y \mapsto f\left(x_{0}, y\right)$ and $x \mapsto f\left(x, y_{0}\right)$ are both continuous on $Y$ and $X$ respectively. If the range space $Z$ is a metric space, with metric $d,$ and $\varepsilon$ is a positive number then we say that $f$ is \tb{$\varepsilon$-jointly continuous}xw at $\left(x_{0}, y_{0}\right) \in X \times Y$ if there exists a product of open sets $U \times V \subseteq X \times Y$ containing $\left(x_{0}, y_{0}\right) \in X \times Y$ such that $d$-diam $f(U \times V) \leq \varepsilon$.





\end{document}
